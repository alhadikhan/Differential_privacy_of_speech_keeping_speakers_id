{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9804778,"sourceType":"datasetVersion","datasetId":6009691},{"sourceId":10032036,"sourceType":"datasetVersion","datasetId":6178775},{"sourceId":10032278,"sourceType":"datasetVersion","datasetId":6178957},{"sourceId":10042684,"sourceType":"datasetVersion","datasetId":6186604},{"sourceId":10042937,"sourceType":"datasetVersion","datasetId":6186791},{"sourceId":10043006,"sourceType":"datasetVersion","datasetId":6186849},{"sourceId":10043086,"sourceType":"datasetVersion","datasetId":6186907},{"sourceId":10043145,"sourceType":"datasetVersion","datasetId":6186947},{"sourceId":10043291,"sourceType":"datasetVersion","datasetId":6187051},{"sourceId":10043690,"sourceType":"datasetVersion","datasetId":6187347}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# e=.041","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport soundfile as sf\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nimport librosa\n\ndef extract_features(file_path, n_mfcc=13):\n    \"\"\"Extract MFCC features from an audio file.\"\"\"\n    audio_signal, sample_rate = sf.read(file_path)\n    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n\ndef apply_laplacian_noise(audio_signal, epsilon):\n    \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n    sensitivity = .041\n    scale = sensitivity / epsilon  # epsilon is the privacy parameter\n    laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n    noisy_signal = audio_signal + laplacian_noise\n    return noisy_signal\n\ndef load_data(dataset_path):\n    X, y = [], []\n    for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n        phrase_path = os.path.join(dataset_path, phrase_type)\n        if not os.path.exists(phrase_path):\n            continue\n        for speaker_id in os.listdir(phrase_path):\n            speaker_path = os.path.join(phrase_path, speaker_id)\n            if not os.path.isdir(speaker_path):\n                continue\n            for audio_file in os.listdir(speaker_path):\n                if audio_file.endswith(\".flac\"):\n                    file_path = os.path.join(speaker_path, audio_file)\n                    features = extract_features(file_path)\n                    if features is not None:\n                        X.append(features)\n                        y.append(speaker_id)  # Speaker ID is used as the label\n    return np.array(X), np.array(y)\n\n# Path to dataset\ndataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\nX, y = load_data(dataset_path)\n\n# Apply Laplacian noise to the MFCC features\nepsilon = .041  # Set the value of epsilon for Laplacian noise\nX_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n\n# Encode speaker IDs (labels) as integers\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n\n# Define and train a Random Forest classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predict on the training set\ny_train_pred = rf_model.predict(X_train)\n\n# Predict on the test set\ny_test_pred = rf_model.predict(X_test)\n\n# Evaluate the model\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprecision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\nprint(f\"Training accuracy: {train_accuracy:.4f}\")\nprint(f\"Validation accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:16:43.478983Z","iopub.execute_input":"2024-11-29T16:16:43.479521Z","iopub.status.idle":"2024-11-29T16:22:39.501717Z","shell.execute_reply.started":"2024-11-29T16:16:43.479468Z","shell.execute_reply":"2024-11-29T16:22:39.500703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# e=.045","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport soundfile as sf\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nimport librosa\n\ndef extract_features(file_path, n_mfcc=13):\n    \"\"\"Extract MFCC features from an audio file.\"\"\"\n    audio_signal, sample_rate = sf.read(file_path)\n    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n\ndef apply_laplacian_noise(audio_signal, epsilon):\n    \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n    sensitivity = 1\n    scale = sensitivity / epsilon  # epsilon is the privacy parameter\n    laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n    noisy_signal = audio_signal + laplacian_noise\n    return noisy_signal\n\ndef load_data(dataset_path):\n    X, y = [], []\n    for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n        phrase_path = os.path.join(dataset_path, phrase_type)\n        if not os.path.exists(phrase_path):\n            continue\n        for speaker_id in os.listdir(phrase_path):\n            speaker_path = os.path.join(phrase_path, speaker_id)\n            if not os.path.isdir(speaker_path):\n                continue\n            for audio_file in os.listdir(speaker_path):\n                if audio_file.endswith(\".flac\"):\n                    file_path = os.path.join(speaker_path, audio_file)\n                    features = extract_features(file_path)\n                    if features is not None:\n                        X.append(features)\n                        y.append(speaker_id)  # Speaker ID is used as the label\n    return np.array(X), np.array(y)\n\n# Path to dataset\ndataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\nX, y = load_data(dataset_path)\n\n# Apply Laplacian noise to the MFCC features\nepsilon = .045  # Set the value of epsilon for Laplacian noise\nX_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n\n# Encode speaker IDs (labels) as integers\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n\n# Define and train a Random Forest classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predict on the training set\ny_train_pred = rf_model.predict(X_train)\n\n# Predict on the test set\ny_test_pred = rf_model.predict(X_test)\n\n# Evaluate the model\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprecision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\nprint(f\"Training accuracy: {train_accuracy:.4f}\")\nprint(f\"Validation accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:23:02.107772Z","iopub.execute_input":"2024-11-29T16:23:02.108810Z","iopub.status.idle":"2024-11-29T16:27:48.371206Z","shell.execute_reply.started":"2024-11-29T16:23:02.108768Z","shell.execute_reply":"2024-11-29T16:27:48.369685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# e=0.048","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport soundfile as sf\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nimport librosa\n\ndef extract_features(file_path, n_mfcc=13):\n    \"\"\"Extract MFCC features from an audio file.\"\"\"\n    audio_signal, sample_rate = sf.read(file_path)\n    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n\ndef apply_laplacian_noise(audio_signal, epsilon):\n    \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n    sensitivity = .048\n    scale = sensitivity / epsilon  # epsilon is the privacy parameter\n    laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n    noisy_signal = audio_signal + laplacian_noise\n    return noisy_signal\n\ndef load_data(dataset_path):\n    X, y = [], []\n    for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n        phrase_path = os.path.join(dataset_path, phrase_type)\n        if not os.path.exists(phrase_path):\n            continue\n        for speaker_id in os.listdir(phrase_path):\n            speaker_path = os.path.join(phrase_path, speaker_id)\n            if not os.path.isdir(speaker_path):\n                continue\n            for audio_file in os.listdir(speaker_path):\n                if audio_file.endswith(\".flac\"):\n                    file_path = os.path.join(speaker_path, audio_file)\n                    features = extract_features(file_path)\n                    if features is not None:\n                        X.append(features)\n                        y.append(speaker_id)  # Speaker ID is used as the label\n    return np.array(X), np.array(y)\n\n# Path to dataset\ndataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\nX, y = load_data(dataset_path)\n\n# Apply Laplacian noise to the MFCC features\nepsilon = .048  # Set the value of epsilon for Laplacian noise\nX_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n\n# Encode speaker IDs (labels) as integers\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n\n# Define and train a Random Forest classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predict on the training set\ny_train_pred = rf_model.predict(X_train)\n\n# Predict on the test set\ny_test_pred = rf_model.predict(X_test)\n\n# Evaluate the model\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\nprecision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\nconf_matrix = confusion_matrix(y_test, y_test_pred)\n\nprint(f\"Training accuracy: {train_accuracy:.4f}\")\nprint(f\"Validation accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# e=without","metadata":{}},{"cell_type":"code","source":"# import os\n# import numpy as np\n# import soundfile as sf\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n# import librosa\n\n# def extract_features(file_path, n_mfcc=13):\n#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n#     audio_signal, sample_rate = sf.read(file_path)\n#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n\n# def load_data(dataset_path):\n#     X, y = [], []\n#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n#         phrase_path = os.path.join(dataset_path, phrase_type)\n#         if not os.path.exists(phrase_path):\n#             continue\n#         for speaker_id in os.listdir(phrase_path):\n#             speaker_path = os.path.join(phrase_path, speaker_id)\n#             if not os.path.isdir(speaker_path):\n#                 continue\n#             for audio_file in os.listdir(speaker_path):\n#                 if audio_file.endswith(\".flac\"):\n#                     file_path = os.path.join(speaker_path, audio_file)\n#                     features = extract_features(file_path)\n#                     if features is not None:\n#                         X.append(features)\n#                         y.append(speaker_id)  # Speaker ID is used as the label\n#     return np.array(X), np.array(y)\n\n# # Path to dataset\n# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n# X, y = load_data(dataset_path)\n\n# # Encode speaker IDs (labels) as integers\n# label_encoder = LabelEncoder()\n# y_encoded = label_encoder.fit_transform(y)\n\n# # Split data into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n\n# # Define and train a Random Forest classifier\n# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n# rf_model.fit(X_train, y_train)\n\n# # Predict on the training set\n# y_train_pred = rf_model.predict(X_train)\n\n# # Predict on the test set\n# y_test_pred = rf_model.predict(X_test)\n\n# # Evaluate the model\n# train_accuracy = accuracy_score(y_train, y_train_pred)\n# test_accuracy = accuracy_score(y_test, y_test_pred)\n# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n# conf_matrix = confusion_matrix(y_test, y_test_pred)\n\n# print(f\"Training accuracy: {train_accuracy:.4f}\")\n# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n# print(f\"Precision: {precision:.4f}\")\n# print(f\"Recall: {recall:.4f}\")\n# print(f\"F1 Score: {f1:.4f}\")\n\n# print(\"Confusion Matrix:\")\n# print(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:35:16.816488Z","iopub.execute_input":"2024-11-29T17:35:16.817019Z","iopub.status.idle":"2024-11-29T17:40:06.585115Z","shell.execute_reply.started":"2024-11-29T17:35:16.816980Z","shell.execute_reply":"2024-11-29T17:40:06.583780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from collections import Counter\n# from sklearn.model_selection import StratifiedKFold\n\n# # Print class distribution\n# class_distribution = Counter(y_encoded)\n# print(\"Class distribution:\", class_distribution)\n\n# # Stratified K-Fold Cross-Validation\n# skf = StratifiedKFold(n_splits=5)\n# for train_index, test_index in skf.split(X, y_encoded):\n#     X_train, X_test = X[train_index], X[test_index]\n#     y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n\n#     # Train the model\n#     rf_model.fit(X_train, y_train)\n\n#     # Predict on the test set\n#     y_test_pred = rf_model.predict(X_test)\n\n#     # Evaluate the model\n#     test_accuracy = accuracy_score(y_test, y_test_pred)\n#     precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n#     conf_matrix = confusion_matrix(y_test, y_test_pred)\n\n#     print(f\"Fold Test accuracy: {test_accuracy:.4f}\")\n#     print(f\"Precision: {precision:.4f}\")\n#     print(f\"Recall: {recall:.4f}\")\n#     print(f\"F1 Score: {f1:.4f}\")\n#     print(\"Confusion Matrix:\")\n#     print(conf_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:40:06.587054Z","iopub.execute_input":"2024-11-29T17:40:06.587352Z","iopub.status.idle":"2024-11-29T17:40:21.905046Z","shell.execute_reply.started":"2024-11-29T17:40:06.587321Z","shell.execute_reply":"2024-11-29T17:40:21.903484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Define the root paths for the 'samePhrase' and 'differentPhrase' directories\n# root_same = '/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/samePhrase'\n# root_diff = '/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/differentPhrase'\n\n# # Function to get all .flac file paths from a directory\n# def collect_audio_file_paths(root_dir):\n#     audio_paths = []\n#     for dirpath, _, filenames in os.walk(root_dir):\n#         for file in filenames:\n#             if file.endswith('.flac'):\n#                 # Append full file path\n#                 audio_paths.append(os.path.join(dirpath, file))\n#     return audio_paths\n\n# # Collect audio paths from both directories\n# audio_paths_same = collect_audio_file_paths(root_same)\n# audio_paths_diff = collect_audio_file_paths(root_diff)\n\n# # Check how many files were found\n# print(f\"Found {len(audio_paths_same)} audio files in 'samePhrase'.\")\n# print(f\"Found {len(audio_paths_diff)} audio files in 'differentPhrase'.\")\n# def get_speaker_id_from_path(file_path):\n#     return int(file_path.split('/')[-2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:16:05.200511Z","iopub.execute_input":"2024-11-29T18:16:05.201731Z","iopub.status.idle":"2024-11-29T18:16:05.648074Z","shell.execute_reply.started":"2024-11-29T18:16:05.201682Z","shell.execute_reply":"2024-11-29T18:16:05.646930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import soundfile as sf\n# from IPython.display import Audio, display\n\n# def apply_laplacian_noise(audio_signal, epsilon):\n#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n#     # Generate Laplacian noise\n#     sensitivity=1\n#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n#     # Add noise to the audio signal\n#     noisy_signal = audio_signal + laplacian_noise\n#     return noisy_signal\n\n# # Set the value of epsilon for Laplacian noise\n# epsilon = 10  # Adjust as needed\n\n# # Filter to get one audio file for each of the first three speaker IDs in both directories\n# first_three_ids = [1, 2, 80, 148]\n# audio_files_to_play = {}\n\n# # Collect one audio file per speaker ID from the samePhrase directory\n# for audio_file in audio_paths_same:\n#     speaker_id = get_speaker_id_from_path(audio_file)\n#     if speaker_id in first_three_ids and speaker_id not in audio_files_to_play:\n#         audio_files_to_play[speaker_id] = {\"samePhrase\": audio_file}\n\n# # Collect one audio file per speaker ID from the differentPhrase directory\n# for audio_file in audio_paths_diff:\n#     speaker_id = get_speaker_id_from_path(audio_file)\n#     if speaker_id in first_three_ids and \"differentPhrase\" not in audio_files_to_play.get(speaker_id, {}):\n#         audio_files_to_play[speaker_id][\"differentPhrase\"] = audio_file\n\n# # Play each audio file (one from samePhrase and one from differentPhrase) for each of the first three speaker IDs\n# for speaker_id, paths in audio_files_to_play.items():\n#     print(f\"Speaker ID: {speaker_id}\")\n#     for phrase_type, audio_file in paths.items():\n#         # Load the audio file\n#         audio_signal, sample_rate = sf.read(audio_file)\n        \n#         # Apply Laplacian noise to the audio signal\n#         noisy_signal = apply_laplacian_noise(audio_signal, epsilon)\n        \n#         # Save the noisy audio to a temporary file or play directly\n#         temp_audio_path = f\"noisy_{phrase_type}_speaker_{speaker_id}.wav\"\n#         sf.write(temp_audio_path, noisy_signal, sample_rate)\n        \n#         print(f\"Playing {phrase_type} audio with Laplacian noise: {temp_audio_path}\")\n#         display(Audio(filename=temp_audio_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:17:04.287472Z","iopub.execute_input":"2024-11-29T18:17:04.287935Z","iopub.status.idle":"2024-11-29T18:17:04.822630Z","shell.execute_reply.started":"2024-11-29T18:17:04.287899Z","shell.execute_reply":"2024-11-29T18:17:04.821257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install noisereduce\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T15:49:22.947728Z","iopub.execute_input":"2024-11-29T15:49:22.948182Z","iopub.status.idle":"2024-11-29T15:49:34.916293Z","shell.execute_reply.started":"2024-11-29T15:49:22.948145Z","shell.execute_reply":"2024-11-29T15:49:34.914537Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import soundfile as sf\n# from IPython.display import Audio, display\n# import noisereduce as nr\n\n# def apply_laplacian_noise(audio_signal, epsilon):\n#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n#     # Generate Laplacian noise\n#     sensitivity = 1\n#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n#     # Add noise to the audio signal\n#     noisy_signal = audio_signal + laplacian_noise\n#     return noisy_signal\n\n# def reduce_noise(audio_signal, sample_rate):\n#     \"\"\"Apply noise reduction to the audio signal using noisereduce library.\"\"\"\n#     reduced_noise_signal = nr.reduce_noise(y=audio_signal, sr=sample_rate)\n#     return reduced_noise_signal\n\n# # Set the value of epsilon for Laplacian noise\n# epsilon = 5  # Adjust as needed\n\n# # Filter to get one audio file for each of the first three speaker IDs in both directories\n# first_three_ids = [1, 2, 80, 148]\n# audio_files_to_play = {}\n\n# # Collect one audio file per speaker ID from the samePhrase directory\n# for audio_file in audio_paths_same:\n#     speaker_id = get_speaker_id_from_path(audio_file)\n#     if speaker_id in first_three_ids and speaker_id not in audio_files_to_play:\n#         audio_files_to_play[speaker_id] = {\"samePhrase\": audio_file}\n\n# # Collect one audio file per speaker ID from the differentPhrase directory\n# for audio_file in audio_paths_diff:\n#     speaker_id = get_speaker_id_from_path(audio_file)\n#     if speaker_id in first_three_ids and \"differentPhrase\" not in audio_files_to_play.get(speaker_id, {}):\n#         audio_files_to_play[speaker_id][\"differentPhrase\"] = audio_file\n\n# # Play each audio file (one from samePhrase and one from differentPhrase) for each of the first three speaker IDs\n# for speaker_id, paths in audio_files_to_play.items():\n#     print(f\"Speaker ID: {speaker_id}\")\n#     for phrase_type, audio_file in paths.items():\n#         # Load the audio file\n#         audio_signal, sample_rate = sf.read(audio_file)\n        \n#         # Apply Laplacian noise to the audio signal\n#         noisy_signal = apply_laplacian_noise(audio_signal, epsilon)\n        \n#         # Reduce noise from the noisy signal\n#         reduced_signal = reduce_noise(noisy_signal, sample_rate)\n        \n#         # Save the noisy and reduced audio to temporary files or play directly\n#         noisy_audio_path = f\"noisy_{phrase_type}_speaker_{speaker_id}.wav\"\n#         reduced_audio_path = f\"reduced_{phrase_type}_speaker_{speaker_id}.wav\"\n        \n#         sf.write(noisy_audio_path, noisy_signal, sample_rate)\n#         sf.write(reduced_audio_path, reduced_signal, sample_rate)\n        \n#         print(f\"Playing {phrase_type} audio with Laplacian noise: {noisy_audio_path}\")\n#         display(Audio(filename=noisy_audio_path))\n        \n#         print(f\"Playing {phrase_type} audio after noise reduction: {reduced_audio_path}\")\n#         display(Audio(filename=reduced_audio_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T15:50:43.983857Z","iopub.execute_input":"2024-11-29T15:50:43.984594Z","iopub.status.idle":"2024-11-29T15:50:46.421272Z","shell.execute_reply.started":"2024-11-29T15:50:43.984529Z","shell.execute_reply":"2024-11-29T15:50:46.419785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# from PIL import Image\n# import numpy as np\n\n# # Path to the base directory containing the folders\n# base_dir = \"/kaggle/input/spectrograms-e0-5-cnn\"\n\n# # Parameters for resizing\n# IMG_SIZE = (224, 224)  # Resize all images to 128x128\n\n# # List to store preprocessed images and corresponding labels\n# data = []  # Stores tuples of (image_array, label)\n\n# # Traverse the directory\n# for folder_name in os.listdir(base_dir):\n#     folder_path = os.path.join(base_dir, folder_name)\n#     if os.path.isdir(folder_path):  # Check if it's a folder\n#         label = int(folder_name)  # Assume folder name is the label\n        \n#         # Load and preprocess each image in the folder\n#         for file_name in os.listdir(folder_path):\n#             file_path = os.path.join(folder_path, file_name)\n#             if file_name.endswith(\".png\"):  # Load only .png files\n#                 try:\n#                     img = Image.open(file_path).convert(\"RGB\")  # Convert to RGB\n#                     img = img.resize(IMG_SIZE)  # Resize the image\n#                     img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n                    \n#                     # Append preprocessed image and label to the data list\n#                     data.append((img_array, label))\n#                 except Exception as e:\n#                     print(f\"Error processing file {file_path}: {e}\")\n\n# # Convert to NumPy arrays if needed for training\n# images, labels = zip(*data)  # Unzip into separate lists\n# images = np.array(images)  # Convert list of images to a NumPy array\n# labels = np.array(labels)  # Convert list of labels to a NumPy array\n\n# # Print summary\n# print(f\"Loaded {len(images)} images with shape {images[0].shape} and {len(labels)} labels.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:43:03.477014Z","iopub.execute_input":"2024-11-27T20:43:03.477457Z","iopub.status.idle":"2024-11-27T20:46:40.588638Z","shell.execute_reply.started":"2024-11-27T20:43:03.477430Z","shell.execute_reply":"2024-11-27T20:46:40.587732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n# from tensorflow.keras.utils import to_categorical\n# from sklearn.model_selection import train_test_split\n\n# # Convert labels to one-hot encoding (assuming labels are integers)\n# labels = to_categorical(labels)\n\n# # Split the data into training and validation sets (80% train, 20% validation)\n# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n\n# # Build the CNN model\n# model = Sequential()\n\n# # Add convolutional layers\n# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n# model.add(MaxPooling2D((2, 2)))\n\n# model.add(Conv2D(64, (3, 3), activation='relu'))\n# model.add(MaxPooling2D((2, 2)))\n\n# model.add(Conv2D(128, (3, 3), activation='relu'))\n# model.add(MaxPooling2D((2, 2)))\n\n# # Flatten the 3D output to 1D\n# model.add(Flatten())\n\n# # Add fully connected layers\n# model.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n\n# model.add(Dense(labels.shape[1], activation='softmax'))  # Output layer (softmax for multi-class classification)\n\n# # Compile the model\n# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# # Train the model\n# history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n\n# # Evaluate the model\n# test_loss, test_accuracy = model.evaluate(X_val, y_val)\n# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n\n# # Optionally, you can plot the training history (accuracy and loss)\n# import matplotlib.pyplot as plt\n\n# # Plot training & validation accuracy values\n# plt.plot(history.history['accuracy'], label='Train Accuracy')\n# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n# plt.title('Model Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.show()\n\n# # Plot training & validation loss values\n# plt.plot(history.history['loss'], label='Train Loss')\n# plt.plot(history.history['val_loss'], label='Validation Loss')\n# plt.title('Model Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:39:48.022800Z","iopub.execute_input":"2024-11-27T20:39:48.023472Z","iopub.status.idle":"2024-11-27T20:43:03.475563Z","shell.execute_reply.started":"2024-11-27T20:39:48.023431Z","shell.execute_reply":"2024-11-27T20:43:03.474620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.applications import VGG19\n# from tensorflow.keras.models import Sequential, Model\n# from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.utils import to_categorical\n# from sklearn.model_selection import train_test_split\n\n# # Convert labels to one-hot encoding (assuming labels are integers)\n# labels = to_categorical(labels)\n\n# # Split the data into training and validation sets (80% train, 20% validation)\n# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n\n# # Load the pre-trained VGG19 model without the top (classifier) layers\n# base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# # Freeze the layers of VGG19 to prevent training on them (only fine-tune the new layers)\n# for layer in base_model.layers:\n#     layer.trainable = False\n\n# # Add custom top layers (fully connected layers)\n# model = Sequential()\n\n# # Add the VGG19 base model\n# model.add(base_model)\n\n# # Add a Global Average Pooling layer\n# model.add(GlobalAveragePooling2D())\n\n# # Add a fully connected (dense) layer with ReLU activation\n# model.add(Dense(512, activation='relu'))\n\n# # Add dropout to reduce overfitting\n# model.add(Dropout(0.5))\n\n# # Output layer with softmax activation (for multi-class classification)\n# model.add(Dense(labels.shape[1], activation='softmax'))\n\n# # Compile the model with Adam optimizer and categorical crossentropy loss function\n# model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# # Print model summary\n# model.summary()\n\n# # Train the model\n# history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n\n# # Evaluate the model\n# test_loss, test_accuracy = model.evaluate(X_val, y_val)\n# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n\n# # Optionally, you can plot the training history (accuracy and loss)\n# import matplotlib.pyplot as plt\n\n# # Plot training & validation accuracy values\n# plt.plot(history.history['accuracy'], label='Train Accuracy')\n# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n# plt.title('VGG19 Model Accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.legend()\n# plt.show()\n\n# # Plot training & validation loss values\n# plt.plot(history.history['loss'], label='Train Loss')\n# plt.plot(history.history['val_loss'], label='Validation Loss')\n# plt.title('VGG19 Model Loss')\n# plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T20:29:23.678978Z","iopub.execute_input":"2024-11-27T20:29:23.679296Z","iopub.status.idle":"2024-11-27T20:29:27.761744Z","shell.execute_reply.started":"2024-11-27T20:29:23.679270Z","shell.execute_reply":"2024-11-27T20:29:27.760554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n\n# # Path to the directory you want to delete (including all its contents)\n# dir_path = '/kaggle/working/'\n\n# # Check if the directory exists and remove it (along with all files and subdirectories)\n# if os.path.exists(dir_path) and os.path.isdir(dir_path):\n#     shutil.rmtree(dir_path)\n#     print(f\"{dir_path} and all its contents have been removed.\")\n# else:\n#     print(f\"The directory {dir_path} does not exist.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T15:05:31.362168Z","iopub.execute_input":"2024-11-27T15:05:31.363278Z","iopub.status.idle":"2024-11-27T15:05:31.370166Z","shell.execute_reply.started":"2024-11-27T15:05:31.363234Z","shell.execute_reply":"2024-11-27T15:05:31.368756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Log_mel_spectogram\n","metadata":{}},{"cell_type":"code","source":"# import os\n# import librosa\n# import librosa.display\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Function to apply Laplacian noise\n# def apply_laplacian_noise(y, epsilon):\n#     sensitivity = 1\n#     scale = sensitivity / epsilon\n#     laplacian_noise = np.random.laplace(0, scale, len(y))\n#     return y + laplacian_noise\n\n# # Function to plot and show Log Mel Spectrogram without axis or numbers\n# def show_log_mel_spectrogram(file_path, epsilon=1000, sr=None):\n#     y, sr = librosa.load(file_path, sr=sr)  # Load audio\n#     y_noisy = apply_laplacian_noise(y, epsilon)  # Apply Laplacian noise\n    \n#     mel_spec = librosa.feature.melspectrogram(y=y_noisy, sr=sr, n_mels=128, fmax=8000)  # Compute Mel spectrogram\n#     log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale (dB)\n\n#     # Plot the spectrogram without axes or numbers\n#     plt.figure(figsize=(10, 4))\n#     librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')\n#     plt.axis('off')  # Hide the axis\n#     plt.tight_layout()\n#     plt.show()  # Show the spectrogram image without saving\n\n# # Function to plot and save Log Mel Spectrogram\n# def save_log_mel_spectrogram(file_path, save_dir, speaker_id, epsilon=1000):\n#     y, sr = librosa.load(file_path, sr=None)  # Load audio\n#     y_noisy = apply_laplacian_noise(y, epsilon)  # Apply Laplacian noise\n    \n#     mel_spec = librosa.feature.melspectrogram(y=y_noisy, sr=sr, n_mels=128, fmax=8000)  # Compute Mel spectrogram\n#     log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale (dB)\n    \n#     # Create the speaker's directory if it doesn't exist\n#     speaker_dir = os.path.join(save_dir, speaker_id)\n#     os.makedirs(speaker_dir, exist_ok=True)\n    \n#     # Generate the filename for the spectrogram image\n#     spectrogram_filename = os.path.splitext(os.path.basename(file_path))[0] + \".png\"\n#     save_path = os.path.join(speaker_dir, spectrogram_filename)\n    \n#     # Plot and save the spectrogram without numbers or axis\n#     plt.figure(figsize=(10, 4))\n#     librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')\n#     plt.axis('off')  # Hide the axis\n#     plt.tight_layout()\n#     plt.savefig(save_path, dpi=300)\n#     plt.close()  # Close the figure to free memory\n\n# # Load dataset and show/save spectrograms\n# def process_and_save_spectrograms(dataset_path, save_dir, epsilon=1000):\n#     image_count = 0  # Counter to show two images\n#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n#         phrase_path = os.path.join(dataset_path, phrase_type)\n#         if not os.path.exists(phrase_path):\n#             continue\n#         for speaker_id in os.listdir(phrase_path):\n#             speaker_path = os.path.join(phrase_path, speaker_id)\n#             if not os.path.isdir(speaker_path):\n#                 continue\n#             for audio_file in os.listdir(speaker_path):\n#                 if audio_file.endswith(\".flac\"):\n#                     file_path = os.path.join(speaker_path, audio_file)\n                    \n#                     # Show two images before saving\n#                     if image_count < 2:\n#                         show_log_mel_spectrogram(file_path)  # Visualize the original\n#                         image_count += 1\n                    \n#                     # Save the spectrogram with noise applied\n#                     save_log_mel_spectrogram(file_path, save_dir, speaker_id, epsilon)\n\n# # Define paths\n# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n# save_dir = \"/kaggle/working/spectrograms_e=1000\"  # Change this path if needed\n\n# # Process and save spectrograms\n# process_and_save_spectrograms(dataset_path, save_dir, epsilon=1000)\n\n# print(f\"Log Mel Spectrograms saved to {save_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:14:29.237753Z","iopub.execute_input":"2024-11-28T16:14:29.238158Z","iopub.status.idle":"2024-11-28T16:39:15.325298Z","shell.execute_reply.started":"2024-11-28T16:14:29.238109Z","shell.execute_reply":"2024-11-28T16:39:15.324033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n\n# # Define the path to the folder and the output zip file\n# folder_path = '/kaggle/working/spectrograms_e=1000'\n# zip_file_path = '/kaggle/working/spectrograms_e=1000'\n\n# # Zip the folder\n# shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n\n# print(f\"Folder '{folder_path}' has been zipped to '{zip_file_path}'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:39:15.327329Z","iopub.execute_input":"2024-11-28T16:39:15.327807Z","iopub.status.idle":"2024-11-28T16:39:51.082636Z","shell.execute_reply.started":"2024-11-28T16:39:15.327771Z","shell.execute_reply":"2024-11-28T16:39:51.081624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'/kaggle/working/spectrograms.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MFCC","metadata":{}},{"cell_type":"code","source":"# import os\n# import librosa\n# import librosa.display\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# # Function to plot and show MFCC Spectrogram without axis or numbers\n# def show_mfcc_spectrogram(file_path, sr=None):\n#     y, sr = librosa.load(file_path, sr=sr)  # Load audio\n#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute MFCC features (13 coefficients)\n    \n#     # Plot the MFCC spectrogram without axes or numbers\n#     plt.figure(figsize=(10, 4))\n#     librosa.display.specshow(mfcc, sr=sr, x_axis='time', cmap='viridis')\n#     plt.axis('off')  # Hide the axis\n#     plt.tight_layout()\n#     plt.show()  # Show the MFCC image without saving\n\n# # Function to plot and save MFCC Spectrogram\n# def save_mfcc_spectrogram(file_path, save_dir, speaker_id):\n#     y, sr = librosa.load(file_path, sr=None)  # Load audio\n#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute MFCC features (13 coefficients)\n    \n#     # Create the speaker's directory if it doesn't exist\n#     speaker_dir = os.path.join(save_dir, speaker_id)\n#     os.makedirs(speaker_dir, exist_ok=True)\n    \n#     # Generate the filename for the MFCC image\n#     mfcc_filename = os.path.splitext(os.path.basename(file_path))[0] + \".png\"\n#     save_path = os.path.join(speaker_dir, mfcc_filename)\n    \n#     # Plot and save the MFCC spectrogram without numbers or axis\n#     plt.figure(figsize=(10, 4))\n#     librosa.display.specshow(mfcc, sr=sr, x_axis='time', cmap='viridis')\n#     plt.axis('off')  # Hide the axis\n#     plt.tight_layout()\n#     plt.savefig(save_path, dpi=300)\n#     plt.close()  # Close the figure to free memory\n\n# # Load dataset and show/save MFCC spectrograms\n# def process_and_save_mfcc_spectrograms(dataset_path, save_dir):\n#     image_count = 0  # Counter to show two images\n#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n#         phrase_path = os.path.join(dataset_path, phrase_type)\n#         if not os.path.exists(phrase_path):\n#             continue\n#         for speaker_id in os.listdir(phrase_path):\n#             speaker_path = os.path.join(phrase_path, speaker_id)\n#             if not os.path.isdir(speaker_path):\n#                 continue\n#             for audio_file in os.listdir(speaker_path):\n#                 if audio_file.endswith(\".flac\"):\n#                     file_path = os.path.join(speaker_path, audio_file)\n                    \n#                     # Show two images before saving\n#                     if image_count < 2:\n#                         show_mfcc_spectrogram(file_path)\n#                         image_count += 1\n                    \n#                     # Save the MFCC spectrogram\n#                     save_mfcc_spectrogram(file_path, save_dir, speaker_id)\n\n# # Define paths\n# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n# save_dir = \"/kaggle/working/mfcc_spectrograms\"  # Change this path if needed\n\n# # Process and save MFCC spectrograms\n# process_and_save_mfcc_spectrograms(dataset_path, save_dir)\n\n# print(f\"MFCC Spectrograms saved to {save_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T15:51:55.013683Z","iopub.execute_input":"2024-11-27T15:51:55.014123Z","iopub.status.idle":"2024-11-27T16:10:01.467672Z","shell.execute_reply.started":"2024-11-27T15:51:55.014088Z","shell.execute_reply":"2024-11-27T16:10:01.466465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Encode labels\n# label_encoder = LabelEncoder()\n# y_encoded = label_encoder.fit_transform(y)\n# y_encoded = tf.keras.utils.to_categorical(y_encoded)\n\n# # Save label encoder classes\n# np.save(\"label_classes.npy\", label_encoder.classes_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:14:11.772268Z","iopub.execute_input":"2024-11-26T16:14:11.772722Z","iopub.status.idle":"2024-11-26T16:14:11.780788Z","shell.execute_reply.started":"2024-11-26T16:14:11.772686Z","shell.execute_reply":"2024-11-26T16:14:11.779548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import librosa\n# from tensorflow.keras.models import load_model\n# from sklearn.preprocessing import LabelEncoder\n\n# # Parameters\n# SR = 16000  # Sampling rate\n# MFCC_FEATURES = 40  # Number of MFCC features\n# MAX_LEN = 100  # Sequence length for padding/truncation\n\n# # Load the trained model\n# model = load_model(\"/kaggle/working/speaker_identity_recognition_model.h5\")\n\n# # Load the label encoder classes\n# label_encoder = LabelEncoder()\n# label_encoder.classes_ = np.load(\"/kaggle/working/label_classes.npy\", allow_pickle=True)  # Load from saved file\n\n# # Feature extraction function\n# def extract_features(file_path):\n#     try:\n#         audio, sr = librosa.load(file_path, sr=SR)\n#         mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=MFCC_FEATURES)\n#         # Pad or truncate\n#         if mfcc.shape[1] > MAX_LEN:\n#             mfcc = mfcc[:, :MAX_LEN]\n#         else:\n#             mfcc = np.pad(mfcc, ((0, 0), (0, MAX_LEN - mfcc.shape[1])), mode=\"constant\")\n#         return mfcc.T\n#     except Exception as e:\n#         print(f\"Error processing {file_path}: {e}\")\n#         return None\n\n# def predict_speaker(audio_path):\n#     features = extract_features(audio_path)\n#     if features is None:\n#         raise ValueError(\"Failed to extract features from the audio file.\")\n    \n#     features = np.expand_dims(features, axis=0)  # Add batch dimension\n#     prediction = model.predict(features)  # Predict using the trained model\n    \n#     # Get the class with the highest probability (argmax) and inverse transform it\n#     speaker_id = label_encoder.inverse_transform([np.argmax(prediction)])  \n#     return speaker_id[0]\n\n# # Test the prediction function\n# audio_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/samePhrase/11/11-1.flac\"  # Replace with the path to your audio file\n\n# try:\n#     predicted_speaker = predict_speaker(audio_path)\n#     print(f\"Predicted Speaker ID: {predicted_speaker}\")\n# except Exception as e:\n#     print(f\"Error during prediction: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T16:15:37.734525Z","iopub.execute_input":"2024-11-26T16:15:37.735822Z","iopub.status.idle":"2024-11-26T16:15:38.340459Z","shell.execute_reply.started":"2024-11-26T16:15:37.735779Z","shell.execute_reply":"2024-11-26T16:15:38.339343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install praat-parselmouth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T20:10:15.936970Z","iopub.execute_input":"2024-11-25T20:10:15.937850Z","iopub.status.idle":"2024-11-25T20:10:29.086916Z","shell.execute_reply.started":"2024-11-25T20:10:15.937808Z","shell.execute_reply":"2024-11-25T20:10:29.085128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import librosa\n# import numpy as np\n# import soundfile as sf\n# import parselmouth\n# import matplotlib.pyplot as plt\n\n# # Extract audio features from the audio file\n# def extract_audio_features(audio_path, sr=16000, duration=10, max_length=300):\n#     # Load audio file\n#     y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n    \n#     # 1. MFCCs and Deltas\n#     mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n#     delta_mfccs = librosa.feature.delta(mfccs)\n#     delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n    \n#     # Combine MFCC features\n#     mfcc_features = np.vstack([mfccs, delta_mfccs, delta2_mfccs])\n    \n#     # 2. Pitch (Fundamental Frequency)\n#     pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n#     pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0  # Mean pitch\n#     pitch_features = np.full((1, mfcc_features.shape[1]), pitch_mean)  # Match time axis\n    \n#     # 3. Energy (RMS)\n#     rms = librosa.feature.rms(y=y).flatten()\n#     rms_features = np.expand_dims(rms, axis=0)  # Add channel dimension\n    \n#     # 4. Log-Mel Spectrogram\n#     mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n#     log_mel_spec = librosa.power_to_db(mel_spec)\n#     log_mel_features = log_mel_spec[:max_length, :]  # Truncate or pad for uniform size\n    \n#     # 5. Formants (F1, F2, F3)\n#     sound = parselmouth.Sound(audio_path)\n#     formant = sound.to_formant_burg()\n#     formants = []\n#     for t in np.linspace(0, sound.duration, num=max_length):\n#         try:\n#             f1 = formant.get_value_at_time(1, t)\n#             f2 = formant.get_value_at_time(2, t)\n#             f3 = formant.get_value_at_time(3, t)\n#             formants.append([f1, f2, f3])\n#         except Exception:\n#             formants.append([0, 0, 0])  # Use 0 for missing values\n    \n#     formants = np.array(formants).T  # Shape (3, max_length)\n    \n#     # Padding or truncating all features to the same length (max_length)\n#     mfcc_features = np.pad(mfcc_features, ((0, 0), (0, max_length - mfcc_features.shape[1])), mode='constant')\n#     pitch_features = np.pad(pitch_features, ((0, 0), (0, max_length - pitch_features.shape[1])), mode='constant')\n#     rms_features = np.pad(rms_features, ((0, 0), (0, max_length - rms_features.shape[1])), mode='constant')\n#     log_mel_features = np.pad(log_mel_features, ((0, 0), (0, max_length - log_mel_features.shape[1])), mode='constant')\n#     formants = np.pad(formants, ((0, 0), (0, max_length - formants.shape[1])), mode='constant')\n    \n#     # Combine all features along the frequency axis\n#     combined_features = np.vstack([mfcc_features, pitch_features, rms_features, log_mel_features, formants])\n    \n#     return combined_features\n\n# # Function to plot features for two speaker IDs\n# def plot_features_for_two_ids(audio_path1, audio_path2, sr=16000, duration=10, max_length=300):\n#     # Extract features for both audio paths\n#     features1 = extract_audio_features(audio_path1, sr=sr, duration=duration, max_length=max_length)\n#     features2 = extract_audio_features(audio_path2, sr=sr, duration=duration, max_length=max_length)\n\n#     # Feature categories\n#     feature_types = [\n#         \"MFCCs and Deltas\",\n#         \"Pitch (Mean)\",\n#         \"Energy (RMS)\",\n#         \"Log-Mel Spectrogram\",\n#         \"Formants (F1, F2, F3)\",\n#     ]\n\n#     # Define feature slices\n#     mfcc_dim = 39  # 13 MFCC + 13 Deltas + 13 Delta-Deltas\n#     pitch_dim = 1\n#     rms_dim = 1\n#     mel_dim = 128\n#     formant_dim = 3\n\n#     # Slice indices\n#     slices = [\n#         (0, mfcc_dim),  # MFCCs\n#         (mfcc_dim, mfcc_dim + pitch_dim),  # Pitch\n#         (mfcc_dim + pitch_dim, mfcc_dim + pitch_dim + rms_dim),  # RMS\n#         (mfcc_dim + pitch_dim + rms_dim, mfcc_dim + pitch_dim + rms_dim + mel_dim),  # Log-Mel\n#         (mfcc_dim + pitch_dim + rms_dim + mel_dim, None),  # Formants\n#     ]\n\n#     # Create subplots\n#     fig, axs = plt.subplots(len(feature_types), 1, figsize=(12, 20))\n#     fig.tight_layout(pad=5.0)\n\n#     for i, (feature_type, (start, end)) in enumerate(zip(feature_types, slices)):\n#         feat1 = features1[start:end, :]\n#         feat2 = features2[start:end, :]\n\n#         # Plot features\n#         if feature_type == \"Log-Mel Spectrogram\":\n#             axs[i].imshow(feat1, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", alpha=0.7)\n#             axs[i].imshow(feat2, aspect=\"auto\", origin=\"lower\", cmap=\"coolwarm\", alpha=0.7)\n#             axs[i].set_title(f\"{feature_type} (Overlap Visualization)\")\n#         elif feature_type == \"Formants (F1, F2, F3)\":\n#             for j, formant in enumerate([\"F1\", \"F2\", \"F3\"]):\n#                 axs[i].plot(feat1[j, :], label=f\"Speaker 1 - {formant}\")\n#                 axs[i].plot(feat2[j, :], label=f\"Speaker 2 - {formant}\", linestyle=\"dashed\")\n#             axs[i].legend()\n#         else:\n#             axs[i].plot(feat1.flatten(), label=\"Speaker 1\")\n#             axs[i].plot(feat2.flatten(), label=\"Speaker 2\", linestyle=\"dashed\")\n#             axs[i].legend()\n\n#         axs[i].set_title(feature_type)\n#         axs[i].set_xlabel(\"Time Frames\")\n#         axs[i].set_ylabel(\"Feature Values\")\n\n#     plt.show()\n\n# # Example usage\n# audio_path1 = all_audio_paths[0]  # Replace with an actual path from a specific speaker ID\n# audio_path2 = all_audio_paths[1]  # Replace with another speaker ID\n# plot_features_for_two_ids(audio_path1, audio_path2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T14:25:54.587088Z","iopub.execute_input":"2024-11-26T14:25:54.587509Z","iopub.status.idle":"2024-11-26T14:25:54.832855Z","shell.execute_reply.started":"2024-11-26T14:25:54.587471Z","shell.execute_reply":"2024-11-26T14:25:54.831070Z"}},"outputs":[],"execution_count":null}]}