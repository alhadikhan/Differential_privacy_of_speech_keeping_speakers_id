{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a39f1be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T21:57:06.571529Z",
     "iopub.status.busy": "2024-12-03T21:57:06.571010Z",
     "iopub.status.idle": "2024-12-03T21:57:06.580524Z",
     "shell.execute_reply": "2024-12-03T21:57:06.579568Z"
    },
    "papermill": {
     "duration": 0.022992,
     "end_time": "2024-12-03T21:57:06.582792",
     "exception": false,
     "start_time": "2024-12-03T21:57:06.559800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(features, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the MFCC features.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, features.shape)\n",
    "#     noisy_features = features + laplacian_noise\n",
    "#     return noisy_features\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# def calculate_eer(y_true, y_scores):\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "#     fnr = 1 - tpr\n",
    "#     eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "#     eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "#     return eer\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Optimize epsilon to balance privacy and accuracy\n",
    "# epsilons = [0.1, 0.5, 1, 2, 5, 10]\n",
    "# results = []\n",
    "\n",
    "# for epsilon in epsilons:\n",
    "#     # Apply Laplacian noise\n",
    "#     X_train = np.array([apply_laplacian_noise(signal, epsilon) for signal in X_train_raw])\n",
    "#     X_test = np.array([apply_laplacian_noise(signal, epsilon) for signal in X_test_raw])\n",
    "    \n",
    "#     # Train and evaluate the model\n",
    "#     rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "    \n",
    "#     y_train_pred = rf_model.predict(X_train)\n",
    "#     y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "#     # Calculate accuracy and EER\n",
    "#     train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "#     conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "#     y_test_scores = rf_model.predict_proba(X_test)[:, 1]\n",
    "#     eer = calculate_eer(y_test, y_test_scores)\n",
    "    \n",
    "#     results.append({\n",
    "#         'epsilon': epsilon,\n",
    "#         'train_accuracy': train_accuracy,\n",
    "#         'test_accuracy': test_accuracy,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1': f1,\n",
    "#         'eer': eer,\n",
    "#         'conf_matrix': conf_matrix\n",
    "#     })\n",
    "\n",
    "# # Find the best epsilon\n",
    "# best_result = min(results, key=lambda x: x['eer'])\n",
    "\n",
    "# # Print results for each epsilon\n",
    "# for result in results:\n",
    "#     print(f\"Epsilon: {result['epsilon']}\")\n",
    "#     print(f\"Training accuracy: {result['train_accuracy']:.4f}\")\n",
    "#     print(f\"Validation accuracy: {result['test_accuracy']:.4f}\")\n",
    "#     print(f\"Precision: {result['precision']:.4f}\")\n",
    "#     print(f\"Recall: {result['recall']:.4f}\")\n",
    "#     print(f\"F1 Score: {result['f1']:.4f}\")\n",
    "#     print(f\"EER: {result['eer']:.4f}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(result['conf_matrix'])\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# # Print the best result\n",
    "# print(\"Best epsilon value based on EER:\")\n",
    "# print(f\"Epsilon: {best_result['epsilon']}\")\n",
    "# print(f\"Training accuracy: {best_result['train_accuracy']:.4f}\")\n",
    "# print(f\"Validation accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "# print(f\"Precision: {best_result['precision']:.4f}\")\n",
    "# print(f\"Recall: {best_result['recall']:.4f}\")\n",
    "# print(f\"F1 Score: {best_result['f1']:.4f}\")\n",
    "# print(f\"EER: {best_result['eer']:.4f}\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(best_result['conf_matrix'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec821af5",
   "metadata": {
    "papermill": {
     "duration": 0.007147,
     "end_time": "2024-12-03T21:57:06.597764",
     "exception": false,
     "start_time": "2024-12-03T21:57:06.590617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc859b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T21:57:06.614344Z",
     "iopub.status.busy": "2024-12-03T21:57:06.614007Z",
     "iopub.status.idle": "2024-12-03T23:10:01.644326Z",
     "shell.execute_reply": "2024-12-03T23:10:01.642855Z"
    },
    "papermill": {
     "duration": 4375.041491,
     "end_time": "2024-12-03T23:10:01.646653",
     "exception": false,
     "start_time": "2024-12-03T21:57:06.605162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0188\n",
      "Precision: 0.0197\n",
      "Recall: 0.0188\n",
      "F1 Score: 0.0188\n",
      "EER: 0.4426\n",
      "Confusion Matrix:\n",
      "[[4 2 1 ... 4 0 1]\n",
      " [2 5 0 ... 1 1 3]\n",
      " [3 2 5 ... 1 3 0]\n",
      " ...\n",
      " [1 3 4 ... 0 3 1]\n",
      " [0 1 2 ... 1 0 2]\n",
      " [2 2 3 ... 1 0 0]]\n",
      "\n",
      "\n",
      "Epsilon: 0.5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0160\n",
      "Precision: 0.0166\n",
      "Recall: 0.0160\n",
      "F1 Score: 0.0160\n",
      "EER: 0.4604\n",
      "Confusion Matrix:\n",
      "[[3 4 1 ... 2 1 2]\n",
      " [5 0 1 ... 3 2 0]\n",
      " [3 3 5 ... 1 1 2]\n",
      " ...\n",
      " [0 0 3 ... 2 2 2]\n",
      " [3 0 1 ... 1 1 5]\n",
      " [2 0 1 ... 3 1 2]]\n",
      "\n",
      "\n",
      "Epsilon: 1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0167\n",
      "Precision: 0.0177\n",
      "Recall: 0.0167\n",
      "F1 Score: 0.0169\n",
      "EER: 0.4494\n",
      "Confusion Matrix:\n",
      "[[1 3 1 ... 0 2 0]\n",
      " [1 0 3 ... 3 1 0]\n",
      " [4 2 2 ... 1 0 3]\n",
      " ...\n",
      " [2 3 4 ... 1 2 0]\n",
      " [5 0 3 ... 2 1 1]\n",
      " [2 2 4 ... 3 3 1]]\n",
      "\n",
      "\n",
      "Epsilon: 2\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0167\n",
      "Precision: 0.0179\n",
      "Recall: 0.0167\n",
      "F1 Score: 0.0168\n",
      "EER: 0.4536\n",
      "Confusion Matrix:\n",
      "[[0 3 2 ... 1 5 1]\n",
      " [1 3 4 ... 2 1 1]\n",
      " [0 3 3 ... 2 1 0]\n",
      " ...\n",
      " [1 5 0 ... 2 2 0]\n",
      " [1 1 0 ... 2 1 3]\n",
      " [3 1 1 ... 1 0 2]]\n",
      "\n",
      "\n",
      "Epsilon: 5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0313\n",
      "Precision: 0.0278\n",
      "Recall: 0.0313\n",
      "F1 Score: 0.0285\n",
      "EER: 0.4502\n",
      "Confusion Matrix:\n",
      "[[2 2 3 ... 2 1 2]\n",
      " [0 5 2 ... 1 1 1]\n",
      " [2 2 1 ... 2 1 2]\n",
      " ...\n",
      " [3 0 1 ... 1 2 0]\n",
      " [2 1 5 ... 2 3 2]\n",
      " [1 3 1 ... 0 0 0]]\n",
      "\n",
      "\n",
      "Epsilon: 10\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0423\n",
      "Precision: 0.0424\n",
      "Recall: 0.0423\n",
      "F1 Score: 0.0418\n",
      "EER: 0.4326\n",
      "Confusion Matrix:\n",
      "[[1 0 3 ... 2 1 2]\n",
      " [3 4 4 ... 3 3 0]\n",
      " [2 4 6 ... 0 0 3]\n",
      " ...\n",
      " [1 1 2 ... 3 0 3]\n",
      " [2 2 3 ... 2 0 1]\n",
      " [2 1 3 ... 2 0 1]]\n",
      "\n",
      "\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.1:\n",
      "Accuracy: 0.0097\n",
      "Precision: 0.0053\n",
      "Recall: 0.0097\n",
      "F1 Score: 0.0030\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [4 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.5:\n",
      "Accuracy: 0.0133\n",
      "Precision: 0.0082\n",
      "Recall: 0.0133\n",
      "F1 Score: 0.0049\n",
      "Confusion Matrix:\n",
      "[[ 7  0  0 ...  0  0  0]\n",
      " [12  0 20 ...  0  0  0]\n",
      " [30  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  2 ...  0  0  0]\n",
      " [ 0  0  7 ...  0  0  0]\n",
      " [15  0  1 ...  0  0  0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 1:\n",
      "Accuracy: 0.0120\n",
      "Precision: 0.0076\n",
      "Recall: 0.0120\n",
      "F1 Score: 0.0041\n",
      "Confusion Matrix:\n",
      "[[  9   0   9 ...   0   0   0]\n",
      " [ 14   0 144 ...   0   0   0]\n",
      " [  1   0  77 ...   0   0   0]\n",
      " ...\n",
      " [  4   0  26 ...   0   0   0]\n",
      " [  0   0 126 ...   0   0   0]\n",
      " [  3   0   7 ...   0   0   0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 2:\n",
      "Accuracy: 0.0171\n",
      "Precision: 0.0090\n",
      "Recall: 0.0171\n",
      "F1 Score: 0.0059\n",
      "Confusion Matrix:\n",
      "[[1 4 0 ... 0 0 0]\n",
      " [8 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [4 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [6 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 5:\n",
      "Accuracy: 0.0167\n",
      "Precision: 0.0003\n",
      "Recall: 0.0167\n",
      "F1 Score: 0.0005\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 10:\n",
      "Accuracy: 0.0169\n",
      "Precision: 0.0026\n",
      "Recall: 0.0169\n",
      "F1 Score: 0.0010\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "import librosa\n",
    "\n",
    "def extract_features_with_noise(file_path, epsilon, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from raw audio with Laplacian noise applied.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    \n",
    "    # Apply Laplacian noise to the raw audio signal\n",
    "    sensitivity = 1  # Adjust based on expected signal range\n",
    "    scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "    laplacian_noise = np.random.laplace(0, scale, audio_signal.shape)\n",
    "    noisy_audio_signal = audio_signal + laplacian_noise\n",
    "    \n",
    "    # Extract MFCC features from the noisy audio\n",
    "    mfccs = librosa.feature.mfcc(y=noisy_audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "def load_audio_mnist_data_with_noise(dataset_path, epsilon):\n",
    "    \"\"\"Load dataset and apply Laplacian noise to raw audio before extracting features.\"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                features = extract_features_with_noise(file_path, epsilon)\n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(speaker_id)  # Use folder name (speaker ID) as label\n",
    "    return np.array(X), np.array(y)\n",
    "    \n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer\n",
    "\n",
    "# Path to dataset\n",
    "dataset_path = \"/kaggle/input/audio-mnist/data\"\n",
    "\n",
    "# Optimize epsilon to balance privacy and accuracy\n",
    "epsilons = [0.1, 0.5, 1, 2, 5, 10]\n",
    "results = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Load data with noise applied to raw audio\n",
    "    X, y = load_audio_mnist_data_with_noise(dataset_path, epsilon)\n",
    "    \n",
    "    # Encode speaker IDs (labels) as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy and EER\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    y_test_scores = rf_model.predict_proba(X_test)[:, 1]\n",
    "    eer = calculate_eer(y_test, y_test_scores)\n",
    "    \n",
    "    results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'eer': eer,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'model': rf_model  # Save the model for raw data evaluation\n",
    "    })\n",
    "\n",
    "# Print results for each epsilon\n",
    "for result in results:\n",
    "    print(f\"Epsilon: {result['epsilon']}\")\n",
    "    print(f\"Training accuracy: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['precision']:.4f}\")\n",
    "    print(f\"Recall: {result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {result['f1']:.4f}\")\n",
    "    print(f\"EER: {result['eer']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(result['conf_matrix'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Load raw audio data (no noise applied)\n",
    "def load_audio_mnist_data(dataset_path):\n",
    "    \"\"\"Load dataset without applying noise.\"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                features = extract_features(file_path)\n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(speaker_id)  # Use folder name (speaker ID) as label\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from raw audio.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "X_raw, y_raw = load_audio_mnist_data(dataset_path)\n",
    "\n",
    "# Encode speaker IDs for the raw data\n",
    "y_raw_encoded = label_encoder.transform(y_raw)  # Use the same encoder from training\n",
    "\n",
    "# Evaluate the model on raw data using the models trained with different epsilons\n",
    "for result in results:\n",
    "    epsilon = result['epsilon']\n",
    "    rf_model = result['model']\n",
    "    \n",
    "    y_raw_pred = rf_model.predict(X_raw)\n",
    "    \n",
    "    raw_accuracy = accuracy_score(y_raw_encoded, y_raw_pred)\n",
    "    raw_precision, raw_recall, raw_f1, _ = precision_recall_fscore_support(y_raw_encoded, y_raw_pred, average='weighted', zero_division=0)\n",
    "    raw_conf_matrix = confusion_matrix(y_raw_encoded, y_raw_pred)\n",
    "    \n",
    "    print(f\"\\nEvaluation on raw audio (no noise applied) for epsilon = {epsilon}:\")\n",
    "    print(f\"Accuracy: {raw_accuracy:.4f}\")\n",
    "    print(f\"Precision: {raw_precision:.4f}\")\n",
    "    print(f\"Recall: {raw_recall:.4f}\")\n",
    "    print(f\"F1 Score: {raw_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(raw_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede29a6",
   "metadata": {
    "papermill": {
     "duration": 0.00779,
     "end_time": "2024-12-03T23:10:01.663915",
     "exception": false,
     "start_time": "2024-12-03T23:10:01.656125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# https://www.kaggle.com/datasets/vjcalling/speaker-recognition-audio-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1e7589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T23:10:01.682303Z",
     "iopub.status.busy": "2024-12-03T23:10:01.681658Z",
     "iopub.status.idle": "2024-12-04T00:05:10.476885Z",
     "shell.execute_reply": "2024-12-04T00:05:10.475060Z"
    },
    "papermill": {
     "duration": 3308.808158,
     "end_time": "2024-12-04T00:05:10.480124",
     "exception": false,
     "start_time": "2024-12-03T23:10:01.671966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0636\n",
      "Precision: 0.0584\n",
      "Recall: 0.0636\n",
      "F1 Score: 0.0552\n",
      "EER: 0.3185\n",
      "Confusion Matrix:\n",
      "[[1 1 1 ... 0 0 0]\n",
      " [2 0 2 ... 0 0 0]\n",
      " [1 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "\n",
      "Epsilon: 0.5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.1193\n",
      "Precision: 0.1152\n",
      "Recall: 0.1193\n",
      "F1 Score: 0.1061\n",
      "EER: 0.3266\n",
      "Confusion Matrix:\n",
      "[[0 2 2 ... 0 0 0]\n",
      " [0 2 1 ... 0 0 0]\n",
      " [1 0 3 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 1 4]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "\n",
      "Epsilon: 1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.2664\n",
      "Precision: 0.2583\n",
      "Recall: 0.2664\n",
      "F1 Score: 0.2476\n",
      "EER: 0.1886\n",
      "Confusion Matrix:\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [3 1 1 ... 0 0 0]\n",
      " [0 0 7 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "\n",
      "Epsilon: 2\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.6262\n",
      "Precision: 0.6544\n",
      "Recall: 0.6262\n",
      "F1 Score: 0.6313\n",
      "EER: 0.0953\n",
      "Confusion Matrix:\n",
      "[[ 4  1  0 ...  0  0  0]\n",
      " [ 1  3  0 ...  0  0  0]\n",
      " [ 0  0  7 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  4  0  1]\n",
      " [ 0  0  0 ...  0 10  0]\n",
      " [ 0  0  0 ...  0  0  4]]\n",
      "\n",
      "\n",
      "Epsilon: 5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.8867\n",
      "Precision: 0.8962\n",
      "Recall: 0.8867\n",
      "F1 Score: 0.8855\n",
      "EER: 0.0101\n",
      "Confusion Matrix:\n",
      "[[ 6  0  0 ...  0  0  0]\n",
      " [ 0  9  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  7  0  0]\n",
      " [ 0  0  0 ...  0 14  0]\n",
      " [ 0  0  0 ...  0  0  6]]\n",
      "\n",
      "\n",
      "Epsilon: 10\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.9304\n",
      "Precision: 0.9345\n",
      "Recall: 0.9304\n",
      "F1 Score: 0.9298\n",
      "EER: 0.0020\n",
      "Confusion Matrix:\n",
      "[[ 6  0  0 ...  0  0  0]\n",
      " [ 0  8  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  7  0  0]\n",
      " [ 0  0  0 ...  0 14  0]\n",
      " [ 0  0  0 ...  0  0  6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.1:\n",
      "Accuracy: 0.0127\n",
      "Precision: 0.0443\n",
      "Recall: 0.0127\n",
      "F1 Score: 0.0067\n",
      "Confusion Matrix:\n",
      "[[ 0 27  0 ...  0  0  0]\n",
      " [ 0  1 19 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0 13  3 ...  0  0  0]\n",
      " [ 0  7 18 ...  0  0  0]\n",
      " [ 0  1 11 ...  0  0  0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.5:\n",
      "Accuracy: 0.0211\n",
      "Precision: 0.0040\n",
      "Recall: 0.0211\n",
      "F1 Score: 0.0057\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 1:\n",
      "Accuracy: 0.0526\n",
      "Precision: 0.0057\n",
      "Recall: 0.0526\n",
      "F1 Score: 0.0084\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 2:\n",
      "Accuracy: 0.0506\n",
      "Precision: 0.0026\n",
      "Recall: 0.0506\n",
      "F1 Score: 0.0050\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 5:\n",
      "Accuracy: 0.0554\n",
      "Precision: 0.0221\n",
      "Recall: 0.0554\n",
      "F1 Score: 0.0129\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 10:\n",
      "Accuracy: 0.0641\n",
      "Precision: 0.0415\n",
      "Recall: 0.0641\n",
      "F1 Score: 0.0208\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "import librosa\n",
    "\n",
    "def extract_features_with_noise(file_path, epsilon, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from an audio file with added Laplacian noise.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    \n",
    "    # Apply Laplacian noise to the raw audio signal\n",
    "    sensitivity = 1  # Adjust based on expected signal range\n",
    "    scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "    laplacian_noise = np.random.laplace(0, scale, audio_signal.shape)\n",
    "    noisy_audio_signal = audio_signal + laplacian_noise\n",
    "    \n",
    "    # Ensure the noisy signal is clipped to valid audio range (-1 to 1 for normalized audio)\n",
    "    noisy_audio_signal = np.clip(noisy_audio_signal, -1.0, 1.0)\n",
    "    \n",
    "    # Extract MFCC features from the noisy audio\n",
    "    mfccs = librosa.feature.mfcc(y=noisy_audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "def load_speaker_recognition_data_with_noise(dataset_path, epsilon):\n",
    "    \"\"\"\n",
    "    Load data from a speaker recognition dataset with noise applied to raw audio.\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset.\n",
    "        epsilon (float): Privacy parameter for noise application.\n",
    "    Returns:\n",
    "        np.array: Features matrix (X).\n",
    "        np.array: Labels array (y).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                try:\n",
    "                    features = extract_features_with_noise(file_path, epsilon=epsilon)\n",
    "                    if features is not None:\n",
    "                        X.append(features)\n",
    "                        y.append(speaker_id)  # Use folder name as label\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = \"/kaggle/input/speaker-recognition-audio-dataset/50_speakers_audio_data\"\n",
    "\n",
    "# Optimize epsilon to balance privacy and accuracy\n",
    "epsilons = [0.1, 0.5, 1, 2, 5, 10]\n",
    "results = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Load data with noise applied to raw audio\n",
    "    X, y = load_speaker_recognition_data_with_noise(dataset_path, epsilon)\n",
    "    \n",
    "    # Encode speaker IDs (labels) as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy and EER\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    y_test_scores = rf_model.predict_proba(X_test)[:, 1]\n",
    "    eer = calculate_eer(y_test, y_test_scores)\n",
    "    \n",
    "    results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'eer': eer,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'model': rf_model  # Save the model for raw data evaluation\n",
    "    })\n",
    "\n",
    "# Print results for each epsilon\n",
    "for result in results:\n",
    "    print(f\"Epsilon: {result['epsilon']}\")\n",
    "    print(f\"Training accuracy: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['precision']:.4f}\")\n",
    "    print(f\"Recall: {result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {result['f1']:.4f}\")\n",
    "    print(f\"EER: {result['eer']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(result['conf_matrix'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Load raw audio data (no noise applied)\n",
    "def load_speaker_recognition_data(dataset_path):\n",
    "    \"\"\"Load dataset without applying noise.\"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                try:\n",
    "                    features = extract_features(file_path)\n",
    "                    if features is not None:\n",
    "                        X.append(features)\n",
    "                        y.append(speaker_id)  # Use folder name as label\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "X_raw, y_raw = load_speaker_recognition_data(dataset_path)\n",
    "\n",
    "# Encode speaker IDs for the raw data\n",
    "y_raw_encoded = label_encoder.transform(y_raw)  # Use the same encoder from training\n",
    "\n",
    "# Evaluate the model on raw data using the models trained with different epsilons\n",
    "for result in results:\n",
    "    epsilon = result['epsilon']\n",
    "    rf_model = result['model']\n",
    "    \n",
    "    y_raw_pred = rf_model.predict(X_raw)\n",
    "    \n",
    "    raw_accuracy = accuracy_score(y_raw_encoded, y_raw_pred)\n",
    "    raw_precision, raw_recall, raw_f1, _ = precision_recall_fscore_support(y_raw_encoded, y_raw_pred, average='weighted', zero_division=0)\n",
    "    raw_conf_matrix = confusion_matrix(y_raw_encoded, y_raw_pred)\n",
    "    \n",
    "    print(f\"\\nEvaluation on raw audio (no noise applied) for epsilon = {epsilon}:\")\n",
    "    print(f\"Accuracy: {raw_accuracy:.4f}\")\n",
    "    print(f\"Precision: {raw_precision:.4f}\")\n",
    "    print(f\"Recall: {raw_recall:.4f}\")\n",
    "    print(f\"F1 Score: {raw_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(raw_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e9834a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:10.542996Z",
     "iopub.status.busy": "2024-12-04T00:05:10.542528Z",
     "iopub.status.idle": "2024-12-04T00:05:10.550383Z",
     "shell.execute_reply": "2024-12-04T00:05:10.549084Z"
    },
    "papermill": {
     "duration": 0.042818,
     "end_time": "2024-12-04T00:05:10.552507",
     "exception": false,
     "start_time": "2024-12-04T00:05:10.509689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Apply Laplacian noise to the MFCC features\n",
    "# epsilon = .5  # Set the value of epsilon for Laplacian noise\n",
    "# X_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88823213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:10.613622Z",
     "iopub.status.busy": "2024-12-04T00:05:10.613209Z",
     "iopub.status.idle": "2024-12-04T00:05:10.620002Z",
     "shell.execute_reply": "2024-12-04T00:05:10.618801Z"
    },
    "papermill": {
     "duration": 0.040115,
     "end_time": "2024-12-04T00:05:10.622172",
     "exception": false,
     "start_time": "2024-12-04T00:05:10.582057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Print model's accuracy on noisy data (already computed earlier)\n",
    "# print(f\"Training accuracy on noisy data: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy on noisy data: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision on noisy data: {precision:.4f}\")\n",
    "# print(f\"Recall on noisy data: {recall:.4f}\")\n",
    "# print(f\"F1 Score on noisy data: {f1:.4f}\")\n",
    "# print(\"Confusion Matrix on noisy data:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # Load raw audio data (no noise applied)\n",
    "# def load_raw_audio_data(dataset_path):\n",
    "#     X_raw, y_raw = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X_raw.append(features)\n",
    "#                         y_raw.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X_raw), np.array(y_raw)\n",
    "\n",
    "# # Load raw audio data\n",
    "# X_raw, y_raw = load_raw_audio_data(dataset_path)\n",
    "\n",
    "# # Encode speaker IDs for the raw data\n",
    "# y_raw_encoded = label_encoder.transform(y_raw)  # Use the same encoder from training\n",
    "\n",
    "# # Predict on raw audio data\n",
    "# y_raw_pred = rf_model.predict(X_raw)\n",
    "\n",
    "# # Evaluate on raw audio data\n",
    "# raw_accuracy = accuracy_score(y_raw_encoded, y_raw_pred)\n",
    "# raw_precision, raw_recall, raw_f1, _ = precision_recall_fscore_support(y_raw_encoded, y_raw_pred, average='weighted')\n",
    "# raw_conf_matrix = confusion_matrix(y_raw_encoded, y_raw_pred)\n",
    "\n",
    "# # Print the evaluation results for raw data\n",
    "# print(\"\\nEvaluation on raw audio (no noise applied):\")\n",
    "# print(f\"Accuracy: {raw_accuracy:.4f}\")\n",
    "# print(f\"Precision: {raw_precision:.4f}\")\n",
    "# print(f\"Recall: {raw_recall:.4f}\")\n",
    "# print(f\"F1 Score: {raw_f1:.4f}\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(raw_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0157f4",
   "metadata": {
    "papermill": {
     "duration": 0.028667,
     "end_time": "2024-12-04T00:05:10.680095",
     "exception": false,
     "start_time": "2024-12-04T00:05:10.651428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# e=.045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b344b0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:10.743111Z",
     "iopub.status.busy": "2024-12-04T00:05:10.742643Z",
     "iopub.status.idle": "2024-12-04T00:05:10.749783Z",
     "shell.execute_reply": "2024-12-04T00:05:10.748637Z"
    },
    "papermill": {
     "duration": 0.042159,
     "end_time": "2024-12-04T00:05:10.752187",
     "exception": false,
     "start_time": "2024-12-04T00:05:10.710028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Apply Laplacian noise to the MFCC features\n",
    "# epsilon = .045  # Set the value of epsilon for Laplacian noise\n",
    "# X_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b8a963",
   "metadata": {
    "papermill": {
     "duration": 0.0292,
     "end_time": "2024-12-04T00:05:10.817845",
     "exception": false,
     "start_time": "2024-12-04T00:05:10.788645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# e=0.048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c654160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:10.879966Z",
     "iopub.status.busy": "2024-12-04T00:05:10.879520Z",
     "iopub.status.idle": "2024-12-04T00:05:10.886742Z",
     "shell.execute_reply": "2024-12-04T00:05:10.885592Z"
    },
    "papermill": {
     "duration": 0.041502,
     "end_time": "2024-12-04T00:05:10.889265",
     "exception": false,
     "start_time": "2024-12-04T00:05:10.847763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Apply Laplacian noise to the MFCC features\n",
    "# epsilon = .048  # Set the value of epsilon for Laplacian noise\n",
    "# X_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c31941",
   "metadata": {
    "papermill": {
     "duration": 0.042696,
     "end_time": "2024-12-04T00:05:10.962479",
     "exception": false,
     "start_time": "2024-12-04T00:05:10.919783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# e=without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343f13bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.048967Z",
     "iopub.status.busy": "2024-12-04T00:05:11.048419Z",
     "iopub.status.idle": "2024-12-04T00:05:11.054988Z",
     "shell.execute_reply": "2024-12-04T00:05:11.053807Z"
    },
    "papermill": {
     "duration": 0.048827,
     "end_time": "2024-12-04T00:05:11.057076",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.008249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9522e875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.118525Z",
     "iopub.status.busy": "2024-12-04T00:05:11.117502Z",
     "iopub.status.idle": "2024-12-04T00:05:11.123671Z",
     "shell.execute_reply": "2024-12-04T00:05:11.122551Z"
    },
    "papermill": {
     "duration": 0.039707,
     "end_time": "2024-12-04T00:05:11.126027",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.086320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Print class distribution\n",
    "# class_distribution = Counter(y_encoded)\n",
    "# print(\"Class distribution:\", class_distribution)\n",
    "\n",
    "# # Stratified K-Fold Cross-Validation\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# for train_index, test_index in skf.split(X, y_encoded):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "#     # Train the model\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "\n",
    "#     # Predict on the test set\n",
    "#     y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "#     conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "#     print(f\"Fold Test accuracy: {test_accuracy:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581eb164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.187107Z",
     "iopub.status.busy": "2024-12-04T00:05:11.186667Z",
     "iopub.status.idle": "2024-12-04T00:05:11.192429Z",
     "shell.execute_reply": "2024-12-04T00:05:11.191145Z"
    },
    "papermill": {
     "duration": 0.039293,
     "end_time": "2024-12-04T00:05:11.194740",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.155447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the root paths for the 'samePhrase' and 'differentPhrase' directories\n",
    "# root_same = '/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/samePhrase'\n",
    "# root_diff = '/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/differentPhrase'\n",
    "\n",
    "# # Function to get all .flac file paths from a directory\n",
    "# def collect_audio_file_paths(root_dir):\n",
    "#     audio_paths = []\n",
    "#     for dirpath, _, filenames in os.walk(root_dir):\n",
    "#         for file in filenames:\n",
    "#             if file.endswith('.flac'):\n",
    "#                 # Append full file path\n",
    "#                 audio_paths.append(os.path.join(dirpath, file))\n",
    "#     return audio_paths\n",
    "\n",
    "# # Collect audio paths from both directories\n",
    "# audio_paths_same = collect_audio_file_paths(root_same)\n",
    "# audio_paths_diff = collect_audio_file_paths(root_diff)\n",
    "\n",
    "# # Check how many files were found\n",
    "# print(f\"Found {len(audio_paths_same)} audio files in 'samePhrase'.\")\n",
    "# print(f\"Found {len(audio_paths_diff)} audio files in 'differentPhrase'.\")\n",
    "# def get_speaker_id_from_path(file_path):\n",
    "#     return int(file_path.split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4260b011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.255350Z",
     "iopub.status.busy": "2024-12-04T00:05:11.254394Z",
     "iopub.status.idle": "2024-12-04T00:05:11.260857Z",
     "shell.execute_reply": "2024-12-04T00:05:11.259632Z"
    },
    "papermill": {
     "duration": 0.038811,
     "end_time": "2024-12-04T00:05:11.263072",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.224261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from IPython.display import Audio, display\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     # Generate Laplacian noise\n",
    "#     sensitivity=1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     # Add noise to the audio signal\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# # Set the value of epsilon for Laplacian noise\n",
    "# epsilon = 10  # Adjust as needed\n",
    "\n",
    "# # Filter to get one audio file for each of the first three speaker IDs in both directories\n",
    "# first_three_ids = [1, 2, 80, 148]\n",
    "# audio_files_to_play = {}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the samePhrase directory\n",
    "# for audio_file in audio_paths_same:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and speaker_id not in audio_files_to_play:\n",
    "#         audio_files_to_play[speaker_id] = {\"samePhrase\": audio_file}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the differentPhrase directory\n",
    "# for audio_file in audio_paths_diff:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and \"differentPhrase\" not in audio_files_to_play.get(speaker_id, {}):\n",
    "#         audio_files_to_play[speaker_id][\"differentPhrase\"] = audio_file\n",
    "\n",
    "# # Play each audio file (one from samePhrase and one from differentPhrase) for each of the first three speaker IDs\n",
    "# for speaker_id, paths in audio_files_to_play.items():\n",
    "#     print(f\"Speaker ID: {speaker_id}\")\n",
    "#     for phrase_type, audio_file in paths.items():\n",
    "#         # Load the audio file\n",
    "#         audio_signal, sample_rate = sf.read(audio_file)\n",
    "        \n",
    "#         # Apply Laplacian noise to the audio signal\n",
    "#         noisy_signal = apply_laplacian_noise(audio_signal, epsilon)\n",
    "        \n",
    "#         # Save the noisy audio to a temporary file or play directly\n",
    "#         temp_audio_path = f\"noisy_{phrase_type}_speaker_{speaker_id}.wav\"\n",
    "#         sf.write(temp_audio_path, noisy_signal, sample_rate)\n",
    "        \n",
    "#         print(f\"Playing {phrase_type} audio with Laplacian noise: {temp_audio_path}\")\n",
    "#         display(Audio(filename=temp_audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf685094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.323161Z",
     "iopub.status.busy": "2024-12-04T00:05:11.322620Z",
     "iopub.status.idle": "2024-12-04T00:05:11.328078Z",
     "shell.execute_reply": "2024-12-04T00:05:11.326791Z"
    },
    "papermill": {
     "duration": 0.038054,
     "end_time": "2024-12-04T00:05:11.330298",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.292244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install noisereduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08da7e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.390000Z",
     "iopub.status.busy": "2024-12-04T00:05:11.389547Z",
     "iopub.status.idle": "2024-12-04T00:05:11.396778Z",
     "shell.execute_reply": "2024-12-04T00:05:11.395464Z"
    },
    "papermill": {
     "duration": 0.039533,
     "end_time": "2024-12-04T00:05:11.398992",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.359459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from IPython.display import Audio, display\n",
    "# import noisereduce as nr\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     # Generate Laplacian noise\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     # Add noise to the audio signal\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def reduce_noise(audio_signal, sample_rate):\n",
    "#     \"\"\"Apply noise reduction to the audio signal using noisereduce library.\"\"\"\n",
    "#     reduced_noise_signal = nr.reduce_noise(y=audio_signal, sr=sample_rate)\n",
    "#     return reduced_noise_signal\n",
    "\n",
    "# # Set the value of epsilon for Laplacian noise\n",
    "# epsilon = 5  # Adjust as needed\n",
    "\n",
    "# # Filter to get one audio file for each of the first three speaker IDs in both directories\n",
    "# first_three_ids = [1, 2, 80, 148]\n",
    "# audio_files_to_play = {}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the samePhrase directory\n",
    "# for audio_file in audio_paths_same:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and speaker_id not in audio_files_to_play:\n",
    "#         audio_files_to_play[speaker_id] = {\"samePhrase\": audio_file}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the differentPhrase directory\n",
    "# for audio_file in audio_paths_diff:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and \"differentPhrase\" not in audio_files_to_play.get(speaker_id, {}):\n",
    "#         audio_files_to_play[speaker_id][\"differentPhrase\"] = audio_file\n",
    "\n",
    "# # Play each audio file (one from samePhrase and one from differentPhrase) for each of the first three speaker IDs\n",
    "# for speaker_id, paths in audio_files_to_play.items():\n",
    "#     print(f\"Speaker ID: {speaker_id}\")\n",
    "#     for phrase_type, audio_file in paths.items():\n",
    "#         # Load the audio file\n",
    "#         audio_signal, sample_rate = sf.read(audio_file)\n",
    "        \n",
    "#         # Apply Laplacian noise to the audio signal\n",
    "#         noisy_signal = apply_laplacian_noise(audio_signal, epsilon)\n",
    "        \n",
    "#         # Reduce noise from the noisy signal\n",
    "#         reduced_signal = reduce_noise(noisy_signal, sample_rate)\n",
    "        \n",
    "#         # Save the noisy and reduced audio to temporary files or play directly\n",
    "#         noisy_audio_path = f\"noisy_{phrase_type}_speaker_{speaker_id}.wav\"\n",
    "#         reduced_audio_path = f\"reduced_{phrase_type}_speaker_{speaker_id}.wav\"\n",
    "        \n",
    "#         sf.write(noisy_audio_path, noisy_signal, sample_rate)\n",
    "#         sf.write(reduced_audio_path, reduced_signal, sample_rate)\n",
    "        \n",
    "#         print(f\"Playing {phrase_type} audio with Laplacian noise: {noisy_audio_path}\")\n",
    "#         display(Audio(filename=noisy_audio_path))\n",
    "        \n",
    "#         print(f\"Playing {phrase_type} audio after noise reduction: {reduced_audio_path}\")\n",
    "#         display(Audio(filename=reduced_audio_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea4d4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.461451Z",
     "iopub.status.busy": "2024-12-04T00:05:11.460358Z",
     "iopub.status.idle": "2024-12-04T00:05:11.466339Z",
     "shell.execute_reply": "2024-12-04T00:05:11.465279Z"
    },
    "papermill": {
     "duration": 0.038501,
     "end_time": "2024-12-04T00:05:11.468425",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.429924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Path to the base directory containing the folders\n",
    "# base_dir = \"/kaggle/input/spectrograms-e0-5-cnn\"\n",
    "\n",
    "# # Parameters for resizing\n",
    "# IMG_SIZE = (224, 224)  # Resize all images to 128x128\n",
    "\n",
    "# # List to store preprocessed images and corresponding labels\n",
    "# data = []  # Stores tuples of (image_array, label)\n",
    "\n",
    "# # Traverse the directory\n",
    "# for folder_name in os.listdir(base_dir):\n",
    "#     folder_path = os.path.join(base_dir, folder_name)\n",
    "#     if os.path.isdir(folder_path):  # Check if it's a folder\n",
    "#         label = int(folder_name)  # Assume folder name is the label\n",
    "        \n",
    "#         # Load and preprocess each image in the folder\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             file_path = os.path.join(folder_path, file_name)\n",
    "#             if file_name.endswith(\".png\"):  # Load only .png files\n",
    "#                 try:\n",
    "#                     img = Image.open(file_path).convert(\"RGB\")  # Convert to RGB\n",
    "#                     img = img.resize(IMG_SIZE)  # Resize the image\n",
    "#                     img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "                    \n",
    "#                     # Append preprocessed image and label to the data list\n",
    "#                     data.append((img_array, label))\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# # Convert to NumPy arrays if needed for training\n",
    "# images, labels = zip(*data)  # Unzip into separate lists\n",
    "# images = np.array(images)  # Convert list of images to a NumPy array\n",
    "# labels = np.array(labels)  # Convert list of labels to a NumPy array\n",
    "\n",
    "# # Print summary\n",
    "# print(f\"Loaded {len(images)} images with shape {images[0].shape} and {len(labels)} labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5828513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.528581Z",
     "iopub.status.busy": "2024-12-04T00:05:11.527937Z",
     "iopub.status.idle": "2024-12-04T00:05:11.534191Z",
     "shell.execute_reply": "2024-12-04T00:05:11.533108Z"
    },
    "papermill": {
     "duration": 0.038879,
     "end_time": "2024-12-04T00:05:11.536352",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.497473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Convert labels to one-hot encoding (assuming labels are integers)\n",
    "# labels = to_categorical(labels)\n",
    "\n",
    "# # Split the data into training and validation sets (80% train, 20% validation)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the CNN model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add convolutional layers\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# # Flatten the 3D output to 1D\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # Add fully connected layers\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "\n",
    "# model.add(Dense(labels.shape[1], activation='softmax'))  # Output layer (softmax for multi-class classification)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Optionally, you can plot the training history (accuracy and loss)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6296af24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.597260Z",
     "iopub.status.busy": "2024-12-04T00:05:11.596807Z",
     "iopub.status.idle": "2024-12-04T00:05:11.603443Z",
     "shell.execute_reply": "2024-12-04T00:05:11.602330Z"
    },
    "papermill": {
     "duration": 0.039474,
     "end_time": "2024-12-04T00:05:11.605611",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.566137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications import VGG19\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Convert labels to one-hot encoding (assuming labels are integers)\n",
    "# labels = to_categorical(labels)\n",
    "\n",
    "# # Split the data into training and validation sets (80% train, 20% validation)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Load the pre-trained VGG19 model without the top (classifier) layers\n",
    "# base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# # Freeze the layers of VGG19 to prevent training on them (only fine-tune the new layers)\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Add custom top layers (fully connected layers)\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the VGG19 base model\n",
    "# model.add(base_model)\n",
    "\n",
    "# # Add a Global Average Pooling layer\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# # Add a fully connected (dense) layer with ReLU activation\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# # Add dropout to reduce overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# # Output layer with softmax activation (for multi-class classification)\n",
    "# model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "\n",
    "# # Compile the model with Adam optimizer and categorical crossentropy loss function\n",
    "# model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# model.summary()\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Optionally, you can plot the training history (accuracy and loss)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('VGG19 Model Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('VGG19 Model Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a9c60cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.666994Z",
     "iopub.status.busy": "2024-12-04T00:05:11.666544Z",
     "iopub.status.idle": "2024-12-04T00:05:11.671246Z",
     "shell.execute_reply": "2024-12-04T00:05:11.670123Z"
    },
    "papermill": {
     "duration": 0.038169,
     "end_time": "2024-12-04T00:05:11.673346",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.635177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Path to the directory you want to delete (including all its contents)\n",
    "# dir_path = '/kaggle/working/'\n",
    "\n",
    "# # Check if the directory exists and remove it (along with all files and subdirectories)\n",
    "# if os.path.exists(dir_path) and os.path.isdir(dir_path):\n",
    "#     shutil.rmtree(dir_path)\n",
    "#     print(f\"{dir_path} and all its contents have been removed.\")\n",
    "# else:\n",
    "#     print(f\"The directory {dir_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc715aec",
   "metadata": {
    "papermill": {
     "duration": 0.030966,
     "end_time": "2024-12-04T00:05:11.733448",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.702482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Log_mel_spectogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af4dcbe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.794022Z",
     "iopub.status.busy": "2024-12-04T00:05:11.793623Z",
     "iopub.status.idle": "2024-12-04T00:05:11.801064Z",
     "shell.execute_reply": "2024-12-04T00:05:11.799773Z"
    },
    "papermill": {
     "duration": 0.040435,
     "end_time": "2024-12-04T00:05:11.803285",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.762850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to apply Laplacian noise\n",
    "# def apply_laplacian_noise(y, epsilon):\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(y))\n",
    "#     return y + laplacian_noise\n",
    "\n",
    "# # Function to plot and show Log Mel Spectrogram without axis or numbers\n",
    "# def show_log_mel_spectrogram(file_path, epsilon=1000, sr=None):\n",
    "#     y, sr = librosa.load(file_path, sr=sr)  # Load audio\n",
    "#     y_noisy = apply_laplacian_noise(y, epsilon)  # Apply Laplacian noise\n",
    "    \n",
    "#     mel_spec = librosa.feature.melspectrogram(y=y_noisy, sr=sr, n_mels=128, fmax=8000)  # Compute Mel spectrogram\n",
    "#     log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale (dB)\n",
    "\n",
    "#     # Plot the spectrogram without axes or numbers\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  # Show the spectrogram image without saving\n",
    "\n",
    "# # Function to plot and save Log Mel Spectrogram\n",
    "# def save_log_mel_spectrogram(file_path, save_dir, speaker_id, epsilon=1000):\n",
    "#     y, sr = librosa.load(file_path, sr=None)  # Load audio\n",
    "#     y_noisy = apply_laplacian_noise(y, epsilon)  # Apply Laplacian noise\n",
    "    \n",
    "#     mel_spec = librosa.feature.melspectrogram(y=y_noisy, sr=sr, n_mels=128, fmax=8000)  # Compute Mel spectrogram\n",
    "#     log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale (dB)\n",
    "    \n",
    "#     # Create the speaker's directory if it doesn't exist\n",
    "#     speaker_dir = os.path.join(save_dir, speaker_id)\n",
    "#     os.makedirs(speaker_dir, exist_ok=True)\n",
    "    \n",
    "#     # Generate the filename for the spectrogram image\n",
    "#     spectrogram_filename = os.path.splitext(os.path.basename(file_path))[0] + \".png\"\n",
    "#     save_path = os.path.join(speaker_dir, spectrogram_filename)\n",
    "    \n",
    "#     # Plot and save the spectrogram without numbers or axis\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300)\n",
    "#     plt.close()  # Close the figure to free memory\n",
    "\n",
    "# # Load dataset and show/save spectrograms\n",
    "# def process_and_save_spectrograms(dataset_path, save_dir, epsilon=1000):\n",
    "#     image_count = 0  # Counter to show two images\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "                    \n",
    "#                     # Show two images before saving\n",
    "#                     if image_count < 2:\n",
    "#                         show_log_mel_spectrogram(file_path)  # Visualize the original\n",
    "#                         image_count += 1\n",
    "                    \n",
    "#                     # Save the spectrogram with noise applied\n",
    "#                     save_log_mel_spectrogram(file_path, save_dir, speaker_id, epsilon)\n",
    "\n",
    "# # Define paths\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# save_dir = \"/kaggle/working/spectrograms_e=1000\"  # Change this path if needed\n",
    "\n",
    "# # Process and save spectrograms\n",
    "# process_and_save_spectrograms(dataset_path, save_dir, epsilon=1000)\n",
    "\n",
    "# print(f\"Log Mel Spectrograms saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b80a994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.864784Z",
     "iopub.status.busy": "2024-12-04T00:05:11.864423Z",
     "iopub.status.idle": "2024-12-04T00:05:11.869520Z",
     "shell.execute_reply": "2024-12-04T00:05:11.868405Z"
    },
    "papermill": {
     "duration": 0.038157,
     "end_time": "2024-12-04T00:05:11.871626",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.833469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Define the path to the folder and the output zip file\n",
    "# folder_path = '/kaggle/working/spectrograms_e=1000'\n",
    "# zip_file_path = '/kaggle/working/spectrograms_e=1000'\n",
    "\n",
    "# # Zip the folder\n",
    "# shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n",
    "\n",
    "# print(f\"Folder '{folder_path}' has been zipped to '{zip_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f536c53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:11.932457Z",
     "iopub.status.busy": "2024-12-04T00:05:11.931649Z",
     "iopub.status.idle": "2024-12-04T00:05:11.936499Z",
     "shell.execute_reply": "2024-12-04T00:05:11.935162Z"
    },
    "papermill": {
     "duration": 0.037856,
     "end_time": "2024-12-04T00:05:11.938644",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.900788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.display import FileLink\n",
    "# FileLink(r'/kaggle/working/spectrograms.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efde42c",
   "metadata": {
    "papermill": {
     "duration": 0.028791,
     "end_time": "2024-12-04T00:05:11.997102",
     "exception": false,
     "start_time": "2024-12-04T00:05:11.968311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd9c4c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:12.058459Z",
     "iopub.status.busy": "2024-12-04T00:05:12.058108Z",
     "iopub.status.idle": "2024-12-04T00:05:12.064577Z",
     "shell.execute_reply": "2024-12-04T00:05:12.063481Z"
    },
    "papermill": {
     "duration": 0.038844,
     "end_time": "2024-12-04T00:05:12.066605",
     "exception": false,
     "start_time": "2024-12-04T00:05:12.027761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to plot and show MFCC Spectrogram without axis or numbers\n",
    "# def show_mfcc_spectrogram(file_path, sr=None):\n",
    "#     y, sr = librosa.load(file_path, sr=sr)  # Load audio\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute MFCC features (13 coefficients)\n",
    "    \n",
    "#     # Plot the MFCC spectrogram without axes or numbers\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(mfcc, sr=sr, x_axis='time', cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  # Show the MFCC image without saving\n",
    "\n",
    "# # Function to plot and save MFCC Spectrogram\n",
    "# def save_mfcc_spectrogram(file_path, save_dir, speaker_id):\n",
    "#     y, sr = librosa.load(file_path, sr=None)  # Load audio\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute MFCC features (13 coefficients)\n",
    "    \n",
    "#     # Create the speaker's directory if it doesn't exist\n",
    "#     speaker_dir = os.path.join(save_dir, speaker_id)\n",
    "#     os.makedirs(speaker_dir, exist_ok=True)\n",
    "    \n",
    "#     # Generate the filename for the MFCC image\n",
    "#     mfcc_filename = os.path.splitext(os.path.basename(file_path))[0] + \".png\"\n",
    "#     save_path = os.path.join(speaker_dir, mfcc_filename)\n",
    "    \n",
    "#     # Plot and save the MFCC spectrogram without numbers or axis\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(mfcc, sr=sr, x_axis='time', cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300)\n",
    "#     plt.close()  # Close the figure to free memory\n",
    "\n",
    "# # Load dataset and show/save MFCC spectrograms\n",
    "# def process_and_save_mfcc_spectrograms(dataset_path, save_dir):\n",
    "#     image_count = 0  # Counter to show two images\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "                    \n",
    "#                     # Show two images before saving\n",
    "#                     if image_count < 2:\n",
    "#                         show_mfcc_spectrogram(file_path)\n",
    "#                         image_count += 1\n",
    "                    \n",
    "#                     # Save the MFCC spectrogram\n",
    "#                     save_mfcc_spectrogram(file_path, save_dir, speaker_id)\n",
    "\n",
    "# # Define paths\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# save_dir = \"/kaggle/working/mfcc_spectrograms\"  # Change this path if needed\n",
    "\n",
    "# # Process and save MFCC spectrograms\n",
    "# process_and_save_mfcc_spectrograms(dataset_path, save_dir)\n",
    "\n",
    "# print(f\"MFCC Spectrograms saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "015d0719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:12.127771Z",
     "iopub.status.busy": "2024-12-04T00:05:12.127402Z",
     "iopub.status.idle": "2024-12-04T00:05:12.132857Z",
     "shell.execute_reply": "2024-12-04T00:05:12.131272Z"
    },
    "papermill": {
     "duration": 0.039093,
     "end_time": "2024-12-04T00:05:12.135526",
     "exception": false,
     "start_time": "2024-12-04T00:05:12.096433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Encode labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "# y_encoded = tf.keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "# # Save label encoder classes\n",
    "# np.save(\"label_classes.npy\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea2effab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:12.196999Z",
     "iopub.status.busy": "2024-12-04T00:05:12.196588Z",
     "iopub.status.idle": "2024-12-04T00:05:12.202565Z",
     "shell.execute_reply": "2024-12-04T00:05:12.201459Z"
    },
    "papermill": {
     "duration": 0.039569,
     "end_time": "2024-12-04T00:05:12.204611",
     "exception": false,
     "start_time": "2024-12-04T00:05:12.165042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import librosa\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Parameters\n",
    "# SR = 16000  # Sampling rate\n",
    "# MFCC_FEATURES = 40  # Number of MFCC features\n",
    "# MAX_LEN = 100  # Sequence length for padding/truncation\n",
    "\n",
    "# # Load the trained model\n",
    "# model = load_model(\"/kaggle/working/speaker_identity_recognition_model.h5\")\n",
    "\n",
    "# # Load the label encoder classes\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.classes_ = np.load(\"/kaggle/working/label_classes.npy\", allow_pickle=True)  # Load from saved file\n",
    "\n",
    "# # Feature extraction function\n",
    "# def extract_features(file_path):\n",
    "#     try:\n",
    "#         audio, sr = librosa.load(file_path, sr=SR)\n",
    "#         mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=MFCC_FEATURES)\n",
    "#         # Pad or truncate\n",
    "#         if mfcc.shape[1] > MAX_LEN:\n",
    "#             mfcc = mfcc[:, :MAX_LEN]\n",
    "#         else:\n",
    "#             mfcc = np.pad(mfcc, ((0, 0), (0, MAX_LEN - mfcc.shape[1])), mode=\"constant\")\n",
    "#         return mfcc.T\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def predict_speaker(audio_path):\n",
    "#     features = extract_features(audio_path)\n",
    "#     if features is None:\n",
    "#         raise ValueError(\"Failed to extract features from the audio file.\")\n",
    "    \n",
    "#     features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
    "#     prediction = model.predict(features)  # Predict using the trained model\n",
    "    \n",
    "#     # Get the class with the highest probability (argmax) and inverse transform it\n",
    "#     speaker_id = label_encoder.inverse_transform([np.argmax(prediction)])  \n",
    "#     return speaker_id[0]\n",
    "\n",
    "# # Test the prediction function\n",
    "# audio_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/samePhrase/11/11-1.flac\"  # Replace with the path to your audio file\n",
    "\n",
    "# try:\n",
    "#     predicted_speaker = predict_speaker(audio_path)\n",
    "#     print(f\"Predicted Speaker ID: {predicted_speaker}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdb86070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:12.264767Z",
     "iopub.status.busy": "2024-12-04T00:05:12.264419Z",
     "iopub.status.idle": "2024-12-04T00:05:12.269201Z",
     "shell.execute_reply": "2024-12-04T00:05:12.268080Z"
    },
    "papermill": {
     "duration": 0.037359,
     "end_time": "2024-12-04T00:05:12.271266",
     "exception": false,
     "start_time": "2024-12-04T00:05:12.233907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32312c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T00:05:12.332447Z",
     "iopub.status.busy": "2024-12-04T00:05:12.332075Z",
     "iopub.status.idle": "2024-12-04T00:05:12.340268Z",
     "shell.execute_reply": "2024-12-04T00:05:12.339085Z"
    },
    "papermill": {
     "duration": 0.041832,
     "end_time": "2024-12-04T00:05:12.342593",
     "exception": false,
     "start_time": "2024-12-04T00:05:12.300761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# import parselmouth\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract audio features from the audio file\n",
    "# def extract_audio_features(audio_path, sr=16000, duration=10, max_length=300):\n",
    "#     # Load audio file\n",
    "#     y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "    \n",
    "#     # 1. MFCCs and Deltas\n",
    "#     mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "#     delta_mfccs = librosa.feature.delta(mfccs)\n",
    "#     delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "    \n",
    "#     # Combine MFCC features\n",
    "#     mfcc_features = np.vstack([mfccs, delta_mfccs, delta2_mfccs])\n",
    "    \n",
    "#     # 2. Pitch (Fundamental Frequency)\n",
    "#     pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "#     pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0  # Mean pitch\n",
    "#     pitch_features = np.full((1, mfcc_features.shape[1]), pitch_mean)  # Match time axis\n",
    "    \n",
    "#     # 3. Energy (RMS)\n",
    "#     rms = librosa.feature.rms(y=y).flatten()\n",
    "#     rms_features = np.expand_dims(rms, axis=0)  # Add channel dimension\n",
    "    \n",
    "#     # 4. Log-Mel Spectrogram\n",
    "#     mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "#     log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "#     log_mel_features = log_mel_spec[:max_length, :]  # Truncate or pad for uniform size\n",
    "    \n",
    "#     # 5. Formants (F1, F2, F3)\n",
    "#     sound = parselmouth.Sound(audio_path)\n",
    "#     formant = sound.to_formant_burg()\n",
    "#     formants = []\n",
    "#     for t in np.linspace(0, sound.duration, num=max_length):\n",
    "#         try:\n",
    "#             f1 = formant.get_value_at_time(1, t)\n",
    "#             f2 = formant.get_value_at_time(2, t)\n",
    "#             f3 = formant.get_value_at_time(3, t)\n",
    "#             formants.append([f1, f2, f3])\n",
    "#         except Exception:\n",
    "#             formants.append([0, 0, 0])  # Use 0 for missing values\n",
    "    \n",
    "#     formants = np.array(formants).T  # Shape (3, max_length)\n",
    "    \n",
    "#     # Padding or truncating all features to the same length (max_length)\n",
    "#     mfcc_features = np.pad(mfcc_features, ((0, 0), (0, max_length - mfcc_features.shape[1])), mode='constant')\n",
    "#     pitch_features = np.pad(pitch_features, ((0, 0), (0, max_length - pitch_features.shape[1])), mode='constant')\n",
    "#     rms_features = np.pad(rms_features, ((0, 0), (0, max_length - rms_features.shape[1])), mode='constant')\n",
    "#     log_mel_features = np.pad(log_mel_features, ((0, 0), (0, max_length - log_mel_features.shape[1])), mode='constant')\n",
    "#     formants = np.pad(formants, ((0, 0), (0, max_length - formants.shape[1])), mode='constant')\n",
    "    \n",
    "#     # Combine all features along the frequency axis\n",
    "#     combined_features = np.vstack([mfcc_features, pitch_features, rms_features, log_mel_features, formants])\n",
    "    \n",
    "#     return combined_features\n",
    "\n",
    "# # Function to plot features for two speaker IDs\n",
    "# def plot_features_for_two_ids(audio_path1, audio_path2, sr=16000, duration=10, max_length=300):\n",
    "#     # Extract features for both audio paths\n",
    "#     features1 = extract_audio_features(audio_path1, sr=sr, duration=duration, max_length=max_length)\n",
    "#     features2 = extract_audio_features(audio_path2, sr=sr, duration=duration, max_length=max_length)\n",
    "\n",
    "#     # Feature categories\n",
    "#     feature_types = [\n",
    "#         \"MFCCs and Deltas\",\n",
    "#         \"Pitch (Mean)\",\n",
    "#         \"Energy (RMS)\",\n",
    "#         \"Log-Mel Spectrogram\",\n",
    "#         \"Formants (F1, F2, F3)\",\n",
    "#     ]\n",
    "\n",
    "#     # Define feature slices\n",
    "#     mfcc_dim = 39  # 13 MFCC + 13 Deltas + 13 Delta-Deltas\n",
    "#     pitch_dim = 1\n",
    "#     rms_dim = 1\n",
    "#     mel_dim = 128\n",
    "#     formant_dim = 3\n",
    "\n",
    "#     # Slice indices\n",
    "#     slices = [\n",
    "#         (0, mfcc_dim),  # MFCCs\n",
    "#         (mfcc_dim, mfcc_dim + pitch_dim),  # Pitch\n",
    "#         (mfcc_dim + pitch_dim, mfcc_dim + pitch_dim + rms_dim),  # RMS\n",
    "#         (mfcc_dim + pitch_dim + rms_dim, mfcc_dim + pitch_dim + rms_dim + mel_dim),  # Log-Mel\n",
    "#         (mfcc_dim + pitch_dim + rms_dim + mel_dim, None),  # Formants\n",
    "#     ]\n",
    "\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(len(feature_types), 1, figsize=(12, 20))\n",
    "#     fig.tight_layout(pad=5.0)\n",
    "\n",
    "#     for i, (feature_type, (start, end)) in enumerate(zip(feature_types, slices)):\n",
    "#         feat1 = features1[start:end, :]\n",
    "#         feat2 = features2[start:end, :]\n",
    "\n",
    "#         # Plot features\n",
    "#         if feature_type == \"Log-Mel Spectrogram\":\n",
    "#             axs[i].imshow(feat1, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", alpha=0.7)\n",
    "#             axs[i].imshow(feat2, aspect=\"auto\", origin=\"lower\", cmap=\"coolwarm\", alpha=0.7)\n",
    "#             axs[i].set_title(f\"{feature_type} (Overlap Visualization)\")\n",
    "#         elif feature_type == \"Formants (F1, F2, F3)\":\n",
    "#             for j, formant in enumerate([\"F1\", \"F2\", \"F3\"]):\n",
    "#                 axs[i].plot(feat1[j, :], label=f\"Speaker 1 - {formant}\")\n",
    "#                 axs[i].plot(feat2[j, :], label=f\"Speaker 2 - {formant}\", linestyle=\"dashed\")\n",
    "#             axs[i].legend()\n",
    "#         else:\n",
    "#             axs[i].plot(feat1.flatten(), label=\"Speaker 1\")\n",
    "#             axs[i].plot(feat2.flatten(), label=\"Speaker 2\", linestyle=\"dashed\")\n",
    "#             axs[i].legend()\n",
    "\n",
    "#         axs[i].set_title(feature_type)\n",
    "#         axs[i].set_xlabel(\"Time Frames\")\n",
    "#         axs[i].set_ylabel(\"Feature Values\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# audio_path1 = all_audio_paths[0]  # Replace with an actual path from a specific speaker ID\n",
    "# audio_path2 = all_audio_paths[1]  # Replace with another speaker ID\n",
    "# plot_features_for_two_ids(audio_path1, audio_path2)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 287521,
     "sourceId": 591084,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1063054,
     "sourceId": 1788543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6009691,
     "sourceId": 9804778,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6178775,
     "sourceId": 10032036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6178957,
     "sourceId": 10032278,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186604,
     "sourceId": 10042684,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186791,
     "sourceId": 10042937,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186849,
     "sourceId": 10043006,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186907,
     "sourceId": 10043086,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186947,
     "sourceId": 10043145,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6187051,
     "sourceId": 10043291,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6187347,
     "sourceId": 10043690,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7689.963157,
   "end_time": "2024-12-04T00:05:13.598534",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-03T21:57:03.635377",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
