{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572ed51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:34:01.822547Z",
     "iopub.status.busy": "2024-12-04T05:34:01.822038Z",
     "iopub.status.idle": "2024-12-04T05:34:01.831388Z",
     "shell.execute_reply": "2024-12-04T05:34:01.830388Z"
    },
    "papermill": {
     "duration": 0.022242,
     "end_time": "2024-12-04T05:34:01.833481",
     "exception": false,
     "start_time": "2024-12-04T05:34:01.811239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(features, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the MFCC features.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, features.shape)\n",
    "#     noisy_features = features + laplacian_noise\n",
    "#     return noisy_features\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# def calculate_eer(y_true, y_scores):\n",
    "#     fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "#     fnr = 1 - tpr\n",
    "#     eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "#     eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "#     return eer\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Optimize epsilon to balance privacy and accuracy\n",
    "# epsilons = [0.1, 0.5, 1, 2, 5, 10]\n",
    "# results = []\n",
    "\n",
    "# for epsilon in epsilons:\n",
    "#     # Apply Laplacian noise\n",
    "#     X_train = np.array([apply_laplacian_noise(signal, epsilon) for signal in X_train_raw])\n",
    "#     X_test = np.array([apply_laplacian_noise(signal, epsilon) for signal in X_test_raw])\n",
    "    \n",
    "#     # Train and evaluate the model\n",
    "#     rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "    \n",
    "#     y_train_pred = rf_model.predict(X_train)\n",
    "#     y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "#     # Calculate accuracy and EER\n",
    "#     train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "#     conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "#     y_test_scores = rf_model.predict_proba(X_test)[:, 1]\n",
    "#     eer = calculate_eer(y_test, y_test_scores)\n",
    "    \n",
    "#     results.append({\n",
    "#         'epsilon': epsilon,\n",
    "#         'train_accuracy': train_accuracy,\n",
    "#         'test_accuracy': test_accuracy,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'f1': f1,\n",
    "#         'eer': eer,\n",
    "#         'conf_matrix': conf_matrix\n",
    "#     })\n",
    "\n",
    "# # Find the best epsilon\n",
    "# best_result = min(results, key=lambda x: x['eer'])\n",
    "\n",
    "# # Print results for each epsilon\n",
    "# for result in results:\n",
    "#     print(f\"Epsilon: {result['epsilon']}\")\n",
    "#     print(f\"Training accuracy: {result['train_accuracy']:.4f}\")\n",
    "#     print(f\"Validation accuracy: {result['test_accuracy']:.4f}\")\n",
    "#     print(f\"Precision: {result['precision']:.4f}\")\n",
    "#     print(f\"Recall: {result['recall']:.4f}\")\n",
    "#     print(f\"F1 Score: {result['f1']:.4f}\")\n",
    "#     print(f\"EER: {result['eer']:.4f}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(result['conf_matrix'])\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# # Print the best result\n",
    "# print(\"Best epsilon value based on EER:\")\n",
    "# print(f\"Epsilon: {best_result['epsilon']}\")\n",
    "# print(f\"Training accuracy: {best_result['train_accuracy']:.4f}\")\n",
    "# print(f\"Validation accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "# print(f\"Precision: {best_result['precision']:.4f}\")\n",
    "# print(f\"Recall: {best_result['recall']:.4f}\")\n",
    "# print(f\"F1 Score: {best_result['f1']:.4f}\")\n",
    "# print(f\"EER: {best_result['eer']:.4f}\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(best_result['conf_matrix'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b8fea",
   "metadata": {
    "papermill": {
     "duration": 0.00741,
     "end_time": "2024-12-04T05:34:01.849191",
     "exception": false,
     "start_time": "2024-12-04T05:34:01.841781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# https://www.kaggle.com/datasets/sripaadsrinivasan/audio-mnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd8cfee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:34:01.866392Z",
     "iopub.status.busy": "2024-12-04T05:34:01.866000Z",
     "iopub.status.idle": "2024-12-04T06:45:35.430332Z",
     "shell.execute_reply": "2024-12-04T06:45:35.429090Z"
    },
    "papermill": {
     "duration": 4293.576135,
     "end_time": "2024-12-04T06:45:35.432979",
     "exception": false,
     "start_time": "2024-12-04T05:34:01.856844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0193\n",
      "Precision: 0.0202\n",
      "Recall: 0.0193\n",
      "F1 Score: 0.0194\n",
      "EER: 0.4577\n",
      "Confusion Matrix:\n",
      "[[3 1 2 ... 3 2 0]\n",
      " [0 3 1 ... 1 7 1]\n",
      " [9 2 3 ... 2 0 0]\n",
      " ...\n",
      " [1 2 1 ... 6 0 1]\n",
      " [1 3 6 ... 2 0 1]\n",
      " [2 6 2 ... 2 0 0]]\n",
      "\n",
      "\n",
      "Epsilon: 0.5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0203\n",
      "Precision: 0.0210\n",
      "Recall: 0.0203\n",
      "F1 Score: 0.0203\n",
      "EER: 0.4504\n",
      "Confusion Matrix:\n",
      "[[4 0 6 ... 1 0 1]\n",
      " [0 3 4 ... 2 0 0]\n",
      " [4 2 1 ... 0 2 3]\n",
      " ...\n",
      " [2 3 1 ... 2 1 4]\n",
      " [0 2 1 ... 3 5 2]\n",
      " [0 3 3 ... 1 0 1]]\n",
      "\n",
      "\n",
      "Epsilon: 1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0218\n",
      "Precision: 0.0216\n",
      "Recall: 0.0218\n",
      "F1 Score: 0.0212\n",
      "EER: 0.4394\n",
      "Confusion Matrix:\n",
      "[[2 0 1 ... 1 2 0]\n",
      " [2 2 1 ... 0 1 2]\n",
      " [2 1 0 ... 1 2 1]\n",
      " ...\n",
      " [2 2 2 ... 1 3 1]\n",
      " [5 0 1 ... 2 0 2]\n",
      " [2 2 3 ... 0 1 1]]\n",
      "\n",
      "\n",
      "Epsilon: 2\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0203\n",
      "Precision: 0.0192\n",
      "Recall: 0.0203\n",
      "F1 Score: 0.0191\n",
      "EER: 0.4553\n",
      "Confusion Matrix:\n",
      "[[3 3 3 ... 3 1 3]\n",
      " [4 4 1 ... 0 2 4]\n",
      " [2 2 1 ... 1 0 1]\n",
      " ...\n",
      " [2 3 1 ... 1 2 1]\n",
      " [2 3 0 ... 5 1 2]\n",
      " [2 0 0 ... 1 0 1]]\n",
      "\n",
      "\n",
      "Epsilon: 5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0335\n",
      "Precision: 0.0276\n",
      "Recall: 0.0335\n",
      "F1 Score: 0.0297\n",
      "EER: 0.4483\n",
      "Confusion Matrix:\n",
      "[[1 3 3 ... 1 1 1]\n",
      " [2 2 3 ... 2 1 2]\n",
      " [1 2 5 ... 2 1 3]\n",
      " ...\n",
      " [3 3 1 ... 2 3 0]\n",
      " [4 3 2 ... 2 1 1]\n",
      " [3 2 1 ... 0 1 4]]\n",
      "\n",
      "\n",
      "Epsilon: 10\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0482\n",
      "Precision: 0.0479\n",
      "Recall: 0.0482\n",
      "F1 Score: 0.0471\n",
      "EER: 0.4472\n",
      "Confusion Matrix:\n",
      "[[1 3 4 ... 3 1 1]\n",
      " [0 1 3 ... 2 2 2]\n",
      " [2 3 6 ... 5 0 2]\n",
      " ...\n",
      " [5 0 3 ... 1 1 2]\n",
      " [3 3 2 ... 5 3 3]\n",
      " [0 6 3 ... 2 2 0]]\n",
      "\n",
      "\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.1:\n",
      "Accuracy: 0.0165\n",
      "Precision: 0.0011\n",
      "Recall: 0.0165\n",
      "F1 Score: 0.0019\n",
      "Confusion Matrix:\n",
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 7 ... 0 0 0]\n",
      " [0 1 2 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.5:\n",
      "Accuracy: 0.0138\n",
      "Precision: 0.0030\n",
      "Recall: 0.0138\n",
      "F1 Score: 0.0033\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 1:\n",
      "Accuracy: 0.0194\n",
      "Precision: 0.0033\n",
      "Recall: 0.0194\n",
      "F1 Score: 0.0039\n",
      "Confusion Matrix:\n",
      "[[ 0  0  1 ...  0  0  0]\n",
      " [ 0  0 29 ...  0  0  0]\n",
      " [ 0  0  5 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  5 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 2:\n",
      "Accuracy: 0.0125\n",
      "Precision: 0.0119\n",
      "Recall: 0.0125\n",
      "F1 Score: 0.0059\n",
      "Confusion Matrix:\n",
      "[[32  0  5 ...  0  0  0]\n",
      " [ 4  2 88 ...  0  0  0]\n",
      " [14  0 37 ...  0  0  0]\n",
      " ...\n",
      " [ 3  2 33 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0 20 ...  0  0  0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 5:\n",
      "Accuracy: 0.0167\n",
      "Precision: 0.0003\n",
      "Recall: 0.0167\n",
      "F1 Score: 0.0005\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 10:\n",
      "Accuracy: 0.0167\n",
      "Precision: 0.0003\n",
      "Recall: 0.0167\n",
      "F1 Score: 0.0005\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "import librosa\n",
    "\n",
    "def extract_features_with_noise(file_path, epsilon, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from raw audio with Laplacian noise applied.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    \n",
    "    # Apply Laplacian noise to the raw audio signal\n",
    "    sensitivity = 1  # Adjust based on expected signal range\n",
    "    scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "    laplacian_noise = np.random.laplace(0, scale, audio_signal.shape)\n",
    "    noisy_audio_signal = audio_signal + laplacian_noise\n",
    "    # Ensure the noisy signal is clipped to valid audio range (-1 to 1 for normalized audio)\n",
    "    noisy_audio_signal = np.clip(noisy_audio_signal, -1.0, 1.0)\n",
    "    \n",
    "    # Extract MFCC features from the noisy audio\n",
    "    mfccs = librosa.feature.mfcc(y=noisy_audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "def load_audio_mnist_data_with_noise(dataset_path, epsilon):\n",
    "    \"\"\"Load dataset and apply Laplacian noise to raw audio before extracting features.\"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                features = extract_features_with_noise(file_path, epsilon)\n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(speaker_id)  # Use folder name (speaker ID) as label\n",
    "    return np.array(X), np.array(y)\n",
    "    \n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer\n",
    "\n",
    "# Path to dataset\n",
    "dataset_path = \"/kaggle/input/audio-mnist/data\"\n",
    "\n",
    "# Optimize epsilon to balance privacy and accuracy\n",
    "epsilons = [0.1, 0.5, 1, 2, 5, 10]\n",
    "results = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Load data with noise applied to raw audio\n",
    "    X, y = load_audio_mnist_data_with_noise(dataset_path, epsilon)\n",
    "    \n",
    "    # Encode speaker IDs (labels) as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy and EER\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    y_test_scores = rf_model.predict_proba(X_test)[:, 1]\n",
    "    eer = calculate_eer(y_test, y_test_scores)\n",
    "    \n",
    "    results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'eer': eer,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'model': rf_model  # Save the model for raw data evaluation\n",
    "    })\n",
    "\n",
    "# Print results for each epsilon\n",
    "for result in results:\n",
    "    print(f\"Epsilon: {result['epsilon']}\")\n",
    "    print(f\"Training accuracy: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['precision']:.4f}\")\n",
    "    print(f\"Recall: {result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {result['f1']:.4f}\")\n",
    "    print(f\"EER: {result['eer']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(result['conf_matrix'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Load raw audio data (no noise applied)\n",
    "def load_audio_mnist_data(dataset_path):\n",
    "    \"\"\"Load dataset without applying noise.\"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                features = extract_features(file_path)\n",
    "                if features is not None:\n",
    "                    X.append(features)\n",
    "                    y.append(speaker_id)  # Use folder name (speaker ID) as label\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from raw audio.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "X_raw, y_raw = load_audio_mnist_data(dataset_path)\n",
    "\n",
    "# Encode speaker IDs for the raw data\n",
    "y_raw_encoded = label_encoder.transform(y_raw)  # Use the same encoder from training\n",
    "\n",
    "# Evaluate the model on raw data using the models trained with different epsilons\n",
    "for result in results:\n",
    "    epsilon = result['epsilon']\n",
    "    rf_model = result['model']\n",
    "    \n",
    "    y_raw_pred = rf_model.predict(X_raw)\n",
    "    \n",
    "    raw_accuracy = accuracy_score(y_raw_encoded, y_raw_pred)\n",
    "    raw_precision, raw_recall, raw_f1, _ = precision_recall_fscore_support(y_raw_encoded, y_raw_pred, average='weighted', zero_division=0)\n",
    "    raw_conf_matrix = confusion_matrix(y_raw_encoded, y_raw_pred)\n",
    "    \n",
    "    print(f\"\\nEvaluation on raw audio (no noise applied) for epsilon = {epsilon}:\")\n",
    "    print(f\"Accuracy: {raw_accuracy:.4f}\")\n",
    "    print(f\"Precision: {raw_precision:.4f}\")\n",
    "    print(f\"Recall: {raw_recall:.4f}\")\n",
    "    print(f\"F1 Score: {raw_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(raw_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a5c33e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:45:35.453342Z",
     "iopub.status.busy": "2024-12-04T06:45:35.452827Z",
     "iopub.status.idle": "2024-12-04T06:45:35.459456Z",
     "shell.execute_reply": "2024-12-04T06:45:35.458354Z"
    },
    "papermill": {
     "duration": 0.019245,
     "end_time": "2024-12-04T06:45:35.461597",
     "exception": false,
     "start_time": "2024-12-04T06:45:35.442352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(file_path, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    \n",
    "    # Optional: Verify audio range\n",
    "    if np.max(audio_signal) > 1.0 or np.min(audio_signal) < -1.0:\n",
    "        print(f\"Warning: Audio signal out of range for file {file_path}. Values will be clipped.\")\n",
    "        audio_signal = np.clip(audio_signal, -1.0, 1.0)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f261b7",
   "metadata": {
    "papermill": {
     "duration": 0.00785,
     "end_time": "2024-12-04T06:45:35.477819",
     "exception": false,
     "start_time": "2024-12-04T06:45:35.469969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# https://www.kaggle.com/datasets/vjcalling/speaker-recognition-audio-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d3c756f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T06:45:35.496363Z",
     "iopub.status.busy": "2024-12-04T06:45:35.495997Z",
     "iopub.status.idle": "2024-12-04T07:40:20.516990Z",
     "shell.execute_reply": "2024-12-04T07:40:20.515380Z"
    },
    "papermill": {
     "duration": 3285.034192,
     "end_time": "2024-12-04T07:40:20.520166",
     "exception": false,
     "start_time": "2024-12-04T06:45:35.485974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon: 0.1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.0716\n",
      "Precision: 0.0744\n",
      "Recall: 0.0716\n",
      "F1 Score: 0.0603\n",
      "EER: 0.3286\n",
      "Confusion Matrix:\n",
      "[[0 1 1 ... 0 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " [3 1 4 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "\n",
      "Epsilon: 0.5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.1133\n",
      "Precision: 0.1068\n",
      "Recall: 0.1133\n",
      "F1 Score: 0.0980\n",
      "EER: 0.2373\n",
      "Confusion Matrix:\n",
      "[[0 0 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 1 0]\n",
      " [0 0 0 ... 0 3 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "\n",
      "Epsilon: 1\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.2942\n",
      "Precision: 0.3189\n",
      "Recall: 0.2942\n",
      "F1 Score: 0.2828\n",
      "EER: 0.2576\n",
      "Confusion Matrix:\n",
      "[[2 0 0 ... 0 0 0]\n",
      " [1 2 0 ... 0 0 0]\n",
      " [0 0 3 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 4 0 1]\n",
      " [0 0 0 ... 0 1 2]\n",
      " [0 0 0 ... 1 1 0]]\n",
      "\n",
      "\n",
      "Epsilon: 2\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.6223\n",
      "Precision: 0.6487\n",
      "Recall: 0.6223\n",
      "F1 Score: 0.6201\n",
      "EER: 0.1136\n",
      "Confusion Matrix:\n",
      "[[5 0 0 ... 0 0 0]\n",
      " [1 3 0 ... 0 0 0]\n",
      " [0 0 7 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 1]\n",
      " [0 0 0 ... 0 9 3]\n",
      " [0 0 0 ... 0 1 1]]\n",
      "\n",
      "\n",
      "Epsilon: 5\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.9026\n",
      "Precision: 0.9115\n",
      "Recall: 0.9026\n",
      "F1 Score: 0.9030\n",
      "EER: 0.0446\n",
      "Confusion Matrix:\n",
      "[[ 6  0  0 ...  0  0  0]\n",
      " [ 0  9  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  7  0  0]\n",
      " [ 0  0  0 ...  0 14  0]\n",
      " [ 0  0  0 ...  0  0  6]]\n",
      "\n",
      "\n",
      "Epsilon: 10\n",
      "Training accuracy: 1.0000\n",
      "Validation accuracy: 0.9344\n",
      "Precision: 0.9389\n",
      "Recall: 0.9344\n",
      "F1 Score: 0.9337\n",
      "EER: 0.0041\n",
      "Confusion Matrix:\n",
      "[[ 6  0  0 ...  0  0  0]\n",
      " [ 0  9  0 ...  0  0  0]\n",
      " [ 0  0 12 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  7  0  0]\n",
      " [ 0  0  0 ...  0 14  0]\n",
      " [ 0  0  0 ...  0  0  6]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1888) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1824) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2656) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2368) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2688) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1728) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2592) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2176) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2752) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1632) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1568) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1696) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2112) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1792) too large for available bit count (1560)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2016) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2144) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2464) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2496) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1664) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (2720) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1984) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1600) too large for available bit count (1568)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1771] error: part2_3_length (1952) too large for available bit count (1568)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.1:\n",
      "Accuracy: 0.0207\n",
      "Precision: 0.0158\n",
      "Recall: 0.0207\n",
      "F1 Score: 0.0095\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [59  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 5  1  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 8  1  0 ...  0  0  0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 0.5:\n",
      "Accuracy: 0.0506\n",
      "Precision: 0.0030\n",
      "Recall: 0.0506\n",
      "F1 Score: 0.0057\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 1:\n",
      "Accuracy: 0.0506\n",
      "Precision: 0.0030\n",
      "Recall: 0.0506\n",
      "F1 Score: 0.0056\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 2:\n",
      "Accuracy: 0.0506\n",
      "Precision: 0.0031\n",
      "Recall: 0.0506\n",
      "F1 Score: 0.0058\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 5:\n",
      "Accuracy: 0.0534\n",
      "Precision: 0.0162\n",
      "Recall: 0.0534\n",
      "F1 Score: 0.0102\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "Evaluation on raw audio (no noise applied) for epsilon = 10:\n",
      "Accuracy: 0.0737\n",
      "Precision: 0.0478\n",
      "Recall: 0.0737\n",
      "F1 Score: 0.0240\n",
      "Confusion Matrix:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_curve\n",
    "import librosa\n",
    "\n",
    "def extract_features_with_noise(file_path, epsilon, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from an audio file with added Laplacian noise.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    \n",
    "    # Apply Laplacian noise to the raw audio signal\n",
    "    sensitivity = 1  # Adjust based on expected signal range\n",
    "    scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "    laplacian_noise = np.random.laplace(0, scale, audio_signal.shape)\n",
    "    noisy_audio_signal = audio_signal + laplacian_noise\n",
    "    \n",
    "    # Ensure the noisy signal is clipped to valid audio range (-1 to 1 for normalized audio)\n",
    "    noisy_audio_signal = np.clip(noisy_audio_signal, -1.0, 1.0)\n",
    "    \n",
    "    # Extract MFCC features from the noisy audio\n",
    "    mfccs = librosa.feature.mfcc(y=noisy_audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "def load_speaker_recognition_data_with_noise(dataset_path, epsilon):\n",
    "    \"\"\"\n",
    "    Load data from a speaker recognition dataset with noise applied to raw audio.\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset.\n",
    "        epsilon (float): Privacy parameter for noise application.\n",
    "    Returns:\n",
    "        np.array: Features matrix (X).\n",
    "        np.array: Labels array (y).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                try:\n",
    "                    features = extract_features_with_noise(file_path, epsilon=epsilon)\n",
    "                    if features is not None:\n",
    "                        X.append(features)\n",
    "                        y.append(speaker_id)  # Use folder name as label\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def calculate_eer(y_true, y_scores):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = thresholds[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    eer = fpr[np.nanargmin(np.abs(fnr - fpr))]\n",
    "    return eer\n",
    "\n",
    "# Path to the dataset\n",
    "dataset_path = \"/kaggle/input/speaker-recognition-audio-dataset/50_speakers_audio_data\"\n",
    "\n",
    "# Optimize epsilon to balance privacy and accuracy\n",
    "epsilons = [0.1, 0.5, 1, 2, 5, 10]\n",
    "results = []\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    # Load data with noise applied to raw audio\n",
    "    X, y = load_speaker_recognition_data_with_noise(dataset_path, epsilon)\n",
    "    \n",
    "    # Encode speaker IDs (labels) as integers\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy and EER\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    y_test_scores = rf_model.predict_proba(X_test)[:, 1]\n",
    "    eer = calculate_eer(y_test, y_test_scores)\n",
    "    \n",
    "    results.append({\n",
    "        'epsilon': epsilon,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'eer': eer,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'model': rf_model  # Save the model for raw data evaluation\n",
    "    })\n",
    "\n",
    "# Print results for each epsilon\n",
    "for result in results:\n",
    "    print(f\"Epsilon: {result['epsilon']}\")\n",
    "    print(f\"Training accuracy: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['precision']:.4f}\")\n",
    "    print(f\"Recall: {result['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {result['f1']:.4f}\")\n",
    "    print(f\"EER: {result['eer']:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(result['conf_matrix'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Load raw audio data (no noise applied)\n",
    "def load_speaker_recognition_data(dataset_path):\n",
    "    \"\"\"Load dataset without applying noise.\"\"\"\n",
    "    X, y = [], []\n",
    "    for speaker_id in os.listdir(dataset_path):\n",
    "        speaker_path = os.path.join(dataset_path, speaker_id)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        for audio_file in os.listdir(speaker_path):\n",
    "            if audio_file.endswith(\".wav\"):\n",
    "                file_path = os.path.join(speaker_path, audio_file)\n",
    "                try:\n",
    "                    features = extract_features(file_path)\n",
    "                    if features is not None:\n",
    "                        X.append(features)\n",
    "                        y.append(speaker_id)  # Use folder name as label\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def extract_features(file_path, n_mfcc=13):\n",
    "    \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "X_raw, y_raw = load_speaker_recognition_data(dataset_path)\n",
    "\n",
    "# Encode speaker IDs for the raw data\n",
    "y_raw_encoded = label_encoder.transform(y_raw)  # Use the same encoder from training\n",
    "\n",
    "# Evaluate the model on raw data using the models trained with different epsilons\n",
    "for result in results:\n",
    "    epsilon = result['epsilon']\n",
    "    rf_model = result['model']\n",
    "    \n",
    "    y_raw_pred = rf_model.predict(X_raw)\n",
    "    \n",
    "    raw_accuracy = accuracy_score(y_raw_encoded, y_raw_pred)\n",
    "    raw_precision, raw_recall, raw_f1, _ = precision_recall_fscore_support(y_raw_encoded, y_raw_pred, average='weighted', zero_division=0)\n",
    "    raw_conf_matrix = confusion_matrix(y_raw_encoded, y_raw_pred)\n",
    "    \n",
    "    print(f\"\\nEvaluation on raw audio (no noise applied) for epsilon = {epsilon}:\")\n",
    "    print(f\"Accuracy: {raw_accuracy:.4f}\")\n",
    "    print(f\"Precision: {raw_precision:.4f}\")\n",
    "    print(f\"Recall: {raw_recall:.4f}\")\n",
    "    print(f\"F1 Score: {raw_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(raw_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d8a89d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:20.583091Z",
     "iopub.status.busy": "2024-12-04T07:40:20.582651Z",
     "iopub.status.idle": "2024-12-04T07:40:20.589759Z",
     "shell.execute_reply": "2024-12-04T07:40:20.588744Z"
    },
    "papermill": {
     "duration": 0.041937,
     "end_time": "2024-12-04T07:40:20.591757",
     "exception": false,
     "start_time": "2024-12-04T07:40:20.549820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Apply Laplacian noise to the MFCC features\n",
    "# epsilon = .5  # Set the value of epsilon for Laplacian noise\n",
    "# X_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59fd4ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:20.652095Z",
     "iopub.status.busy": "2024-12-04T07:40:20.651673Z",
     "iopub.status.idle": "2024-12-04T07:40:20.657670Z",
     "shell.execute_reply": "2024-12-04T07:40:20.656763Z"
    },
    "papermill": {
     "duration": 0.03866,
     "end_time": "2024-12-04T07:40:20.659692",
     "exception": false,
     "start_time": "2024-12-04T07:40:20.621032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Print model's accuracy on noisy data (already computed earlier)\n",
    "# print(f\"Training accuracy on noisy data: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy on noisy data: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision on noisy data: {precision:.4f}\")\n",
    "# print(f\"Recall on noisy data: {recall:.4f}\")\n",
    "# print(f\"F1 Score on noisy data: {f1:.4f}\")\n",
    "# print(\"Confusion Matrix on noisy data:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # Load raw audio data (no noise applied)\n",
    "# def load_raw_audio_data(dataset_path):\n",
    "#     X_raw, y_raw = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X_raw.append(features)\n",
    "#                         y_raw.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X_raw), np.array(y_raw)\n",
    "\n",
    "# # Load raw audio data\n",
    "# X_raw, y_raw = load_raw_audio_data(dataset_path)\n",
    "\n",
    "# # Encode speaker IDs for the raw data\n",
    "# y_raw_encoded = label_encoder.transform(y_raw)  # Use the same encoder from training\n",
    "\n",
    "# # Predict on raw audio data\n",
    "# y_raw_pred = rf_model.predict(X_raw)\n",
    "\n",
    "# # Evaluate on raw audio data\n",
    "# raw_accuracy = accuracy_score(y_raw_encoded, y_raw_pred)\n",
    "# raw_precision, raw_recall, raw_f1, _ = precision_recall_fscore_support(y_raw_encoded, y_raw_pred, average='weighted')\n",
    "# raw_conf_matrix = confusion_matrix(y_raw_encoded, y_raw_pred)\n",
    "\n",
    "# # Print the evaluation results for raw data\n",
    "# print(\"\\nEvaluation on raw audio (no noise applied):\")\n",
    "# print(f\"Accuracy: {raw_accuracy:.4f}\")\n",
    "# print(f\"Precision: {raw_precision:.4f}\")\n",
    "# print(f\"Recall: {raw_recall:.4f}\")\n",
    "# print(f\"F1 Score: {raw_f1:.4f}\")\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(raw_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eac643",
   "metadata": {
    "papermill": {
     "duration": 0.029204,
     "end_time": "2024-12-04T07:40:20.718055",
     "exception": false,
     "start_time": "2024-12-04T07:40:20.688851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# e=.045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e3765a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:20.779543Z",
     "iopub.status.busy": "2024-12-04T07:40:20.779108Z",
     "iopub.status.idle": "2024-12-04T07:40:20.786133Z",
     "shell.execute_reply": "2024-12-04T07:40:20.785193Z"
    },
    "papermill": {
     "duration": 0.041049,
     "end_time": "2024-12-04T07:40:20.787990",
     "exception": false,
     "start_time": "2024-12-04T07:40:20.746941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Apply Laplacian noise to the MFCC features\n",
    "# epsilon = .045  # Set the value of epsilon for Laplacian noise\n",
    "# X_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc63f87",
   "metadata": {
    "papermill": {
     "duration": 0.029245,
     "end_time": "2024-12-04T07:40:20.846442",
     "exception": false,
     "start_time": "2024-12-04T07:40:20.817197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# e=0.048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f17ab9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:20.906928Z",
     "iopub.status.busy": "2024-12-04T07:40:20.906525Z",
     "iopub.status.idle": "2024-12-04T07:40:20.913398Z",
     "shell.execute_reply": "2024-12-04T07:40:20.912257Z"
    },
    "papermill": {
     "duration": 0.040094,
     "end_time": "2024-12-04T07:40:20.915456",
     "exception": false,
     "start_time": "2024-12-04T07:40:20.875362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Apply Laplacian noise to the MFCC features\n",
    "# epsilon = .048  # Set the value of epsilon for Laplacian noise\n",
    "# X_noisy = np.array([apply_laplacian_noise(signal, epsilon) for signal in X])\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_noisy, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e57bb4",
   "metadata": {
    "papermill": {
     "duration": 0.028779,
     "end_time": "2024-12-04T07:40:20.973532",
     "exception": false,
     "start_time": "2024-12-04T07:40:20.944753",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# e=without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c604a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.033587Z",
     "iopub.status.busy": "2024-12-04T07:40:21.033185Z",
     "iopub.status.idle": "2024-12-04T07:40:21.039760Z",
     "shell.execute_reply": "2024-12-04T07:40:21.038683Z"
    },
    "papermill": {
     "duration": 0.039337,
     "end_time": "2024-12-04T07:40:21.041895",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.002558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "# import librosa\n",
    "\n",
    "# def extract_features(file_path, n_mfcc=13):\n",
    "#     \"\"\"Extract MFCC features from an audio file.\"\"\"\n",
    "#     audio_signal, sample_rate = sf.read(file_path)\n",
    "#     mfccs = librosa.feature.mfcc(y=audio_signal, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "#     return np.mean(mfccs.T, axis=0)  # Use the mean of MFCCs over time\n",
    "\n",
    "# def load_data(dataset_path):\n",
    "#     X, y = [], []\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "#                     features = extract_features(file_path)\n",
    "#                     if features is not None:\n",
    "#                         X.append(features)\n",
    "#                         y.append(speaker_id)  # Speaker ID is used as the label\n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# # Path to dataset\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# X, y = load_data(dataset_path)\n",
    "\n",
    "# # Encode speaker IDs (labels) as integers\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define and train a Random Forest classifier\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "# conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75dcca34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.103444Z",
     "iopub.status.busy": "2024-12-04T07:40:21.103041Z",
     "iopub.status.idle": "2024-12-04T07:40:21.108484Z",
     "shell.execute_reply": "2024-12-04T07:40:21.107419Z"
    },
    "papermill": {
     "duration": 0.03921,
     "end_time": "2024-12-04T07:40:21.110549",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.071339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Print class distribution\n",
    "# class_distribution = Counter(y_encoded)\n",
    "# print(\"Class distribution:\", class_distribution)\n",
    "\n",
    "# # Stratified K-Fold Cross-Validation\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# for train_index, test_index in skf.split(X, y_encoded):\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "#     # Train the model\n",
    "#     rf_model.fit(X_train, y_train)\n",
    "\n",
    "#     # Predict on the test set\n",
    "#     y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='weighted')\n",
    "#     conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "#     print(f\"Fold Test accuracy: {test_accuracy:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540d195b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.172004Z",
     "iopub.status.busy": "2024-12-04T07:40:21.171596Z",
     "iopub.status.idle": "2024-12-04T07:40:21.177019Z",
     "shell.execute_reply": "2024-12-04T07:40:21.175917Z"
    },
    "papermill": {
     "duration": 0.03891,
     "end_time": "2024-12-04T07:40:21.179167",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.140257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Define the root paths for the 'samePhrase' and 'differentPhrase' directories\n",
    "# root_same = '/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/samePhrase'\n",
    "# root_diff = '/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/differentPhrase'\n",
    "\n",
    "# # Function to get all .flac file paths from a directory\n",
    "# def collect_audio_file_paths(root_dir):\n",
    "#     audio_paths = []\n",
    "#     for dirpath, _, filenames in os.walk(root_dir):\n",
    "#         for file in filenames:\n",
    "#             if file.endswith('.flac'):\n",
    "#                 # Append full file path\n",
    "#                 audio_paths.append(os.path.join(dirpath, file))\n",
    "#     return audio_paths\n",
    "\n",
    "# # Collect audio paths from both directories\n",
    "# audio_paths_same = collect_audio_file_paths(root_same)\n",
    "# audio_paths_diff = collect_audio_file_paths(root_diff)\n",
    "\n",
    "# # Check how many files were found\n",
    "# print(f\"Found {len(audio_paths_same)} audio files in 'samePhrase'.\")\n",
    "# print(f\"Found {len(audio_paths_diff)} audio files in 'differentPhrase'.\")\n",
    "# def get_speaker_id_from_path(file_path):\n",
    "#     return int(file_path.split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2971da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.240422Z",
     "iopub.status.busy": "2024-12-04T07:40:21.239478Z",
     "iopub.status.idle": "2024-12-04T07:40:21.245699Z",
     "shell.execute_reply": "2024-12-04T07:40:21.244644Z"
    },
    "papermill": {
     "duration": 0.039011,
     "end_time": "2024-12-04T07:40:21.247911",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.208900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from IPython.display import Audio, display\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     # Generate Laplacian noise\n",
    "#     sensitivity=1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     # Add noise to the audio signal\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# # Set the value of epsilon for Laplacian noise\n",
    "# epsilon = 10  # Adjust as needed\n",
    "\n",
    "# # Filter to get one audio file for each of the first three speaker IDs in both directories\n",
    "# first_three_ids = [1, 2, 80, 148]\n",
    "# audio_files_to_play = {}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the samePhrase directory\n",
    "# for audio_file in audio_paths_same:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and speaker_id not in audio_files_to_play:\n",
    "#         audio_files_to_play[speaker_id] = {\"samePhrase\": audio_file}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the differentPhrase directory\n",
    "# for audio_file in audio_paths_diff:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and \"differentPhrase\" not in audio_files_to_play.get(speaker_id, {}):\n",
    "#         audio_files_to_play[speaker_id][\"differentPhrase\"] = audio_file\n",
    "\n",
    "# # Play each audio file (one from samePhrase and one from differentPhrase) for each of the first three speaker IDs\n",
    "# for speaker_id, paths in audio_files_to_play.items():\n",
    "#     print(f\"Speaker ID: {speaker_id}\")\n",
    "#     for phrase_type, audio_file in paths.items():\n",
    "#         # Load the audio file\n",
    "#         audio_signal, sample_rate = sf.read(audio_file)\n",
    "        \n",
    "#         # Apply Laplacian noise to the audio signal\n",
    "#         noisy_signal = apply_laplacian_noise(audio_signal, epsilon)\n",
    "        \n",
    "#         # Save the noisy audio to a temporary file or play directly\n",
    "#         temp_audio_path = f\"noisy_{phrase_type}_speaker_{speaker_id}.wav\"\n",
    "#         sf.write(temp_audio_path, noisy_signal, sample_rate)\n",
    "        \n",
    "#         print(f\"Playing {phrase_type} audio with Laplacian noise: {temp_audio_path}\")\n",
    "#         display(Audio(filename=temp_audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0895db79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.308393Z",
     "iopub.status.busy": "2024-12-04T07:40:21.307983Z",
     "iopub.status.idle": "2024-12-04T07:40:21.313258Z",
     "shell.execute_reply": "2024-12-04T07:40:21.312161Z"
    },
    "papermill": {
     "duration": 0.038089,
     "end_time": "2024-12-04T07:40:21.315304",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.277215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install noisereduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4fad9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.376488Z",
     "iopub.status.busy": "2024-12-04T07:40:21.375599Z",
     "iopub.status.idle": "2024-12-04T07:40:21.382713Z",
     "shell.execute_reply": "2024-12-04T07:40:21.381553Z"
    },
    "papermill": {
     "duration": 0.040169,
     "end_time": "2024-12-04T07:40:21.384829",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.344660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# from IPython.display import Audio, display\n",
    "# import noisereduce as nr\n",
    "\n",
    "# def apply_laplacian_noise(audio_signal, epsilon):\n",
    "#     \"\"\"Apply Laplacian noise to the audio signal.\"\"\"\n",
    "#     # Generate Laplacian noise\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon  # epsilon is the privacy parameter\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(audio_signal))\n",
    "#     # Add noise to the audio signal\n",
    "#     noisy_signal = audio_signal + laplacian_noise\n",
    "#     return noisy_signal\n",
    "\n",
    "# def reduce_noise(audio_signal, sample_rate):\n",
    "#     \"\"\"Apply noise reduction to the audio signal using noisereduce library.\"\"\"\n",
    "#     reduced_noise_signal = nr.reduce_noise(y=audio_signal, sr=sample_rate)\n",
    "#     return reduced_noise_signal\n",
    "\n",
    "# # Set the value of epsilon for Laplacian noise\n",
    "# epsilon = 5  # Adjust as needed\n",
    "\n",
    "# # Filter to get one audio file for each of the first three speaker IDs in both directories\n",
    "# first_three_ids = [1, 2, 80, 148]\n",
    "# audio_files_to_play = {}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the samePhrase directory\n",
    "# for audio_file in audio_paths_same:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and speaker_id not in audio_files_to_play:\n",
    "#         audio_files_to_play[speaker_id] = {\"samePhrase\": audio_file}\n",
    "\n",
    "# # Collect one audio file per speaker ID from the differentPhrase directory\n",
    "# for audio_file in audio_paths_diff:\n",
    "#     speaker_id = get_speaker_id_from_path(audio_file)\n",
    "#     if speaker_id in first_three_ids and \"differentPhrase\" not in audio_files_to_play.get(speaker_id, {}):\n",
    "#         audio_files_to_play[speaker_id][\"differentPhrase\"] = audio_file\n",
    "\n",
    "# # Play each audio file (one from samePhrase and one from differentPhrase) for each of the first three speaker IDs\n",
    "# for speaker_id, paths in audio_files_to_play.items():\n",
    "#     print(f\"Speaker ID: {speaker_id}\")\n",
    "#     for phrase_type, audio_file in paths.items():\n",
    "#         # Load the audio file\n",
    "#         audio_signal, sample_rate = sf.read(audio_file)\n",
    "        \n",
    "#         # Apply Laplacian noise to the audio signal\n",
    "#         noisy_signal = apply_laplacian_noise(audio_signal, epsilon)\n",
    "        \n",
    "#         # Reduce noise from the noisy signal\n",
    "#         reduced_signal = reduce_noise(noisy_signal, sample_rate)\n",
    "        \n",
    "#         # Save the noisy and reduced audio to temporary files or play directly\n",
    "#         noisy_audio_path = f\"noisy_{phrase_type}_speaker_{speaker_id}.wav\"\n",
    "#         reduced_audio_path = f\"reduced_{phrase_type}_speaker_{speaker_id}.wav\"\n",
    "        \n",
    "#         sf.write(noisy_audio_path, noisy_signal, sample_rate)\n",
    "#         sf.write(reduced_audio_path, reduced_signal, sample_rate)\n",
    "        \n",
    "#         print(f\"Playing {phrase_type} audio with Laplacian noise: {noisy_audio_path}\")\n",
    "#         display(Audio(filename=noisy_audio_path))\n",
    "        \n",
    "#         print(f\"Playing {phrase_type} audio after noise reduction: {reduced_audio_path}\")\n",
    "#         display(Audio(filename=reduced_audio_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53b9518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.445417Z",
     "iopub.status.busy": "2024-12-04T07:40:21.445017Z",
     "iopub.status.idle": "2024-12-04T07:40:21.450925Z",
     "shell.execute_reply": "2024-12-04T07:40:21.449742Z"
    },
    "papermill": {
     "duration": 0.038929,
     "end_time": "2024-12-04T07:40:21.453027",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.414098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# # Path to the base directory containing the folders\n",
    "# base_dir = \"/kaggle/input/spectrograms-e0-5-cnn\"\n",
    "\n",
    "# # Parameters for resizing\n",
    "# IMG_SIZE = (224, 224)  # Resize all images to 128x128\n",
    "\n",
    "# # List to store preprocessed images and corresponding labels\n",
    "# data = []  # Stores tuples of (image_array, label)\n",
    "\n",
    "# # Traverse the directory\n",
    "# for folder_name in os.listdir(base_dir):\n",
    "#     folder_path = os.path.join(base_dir, folder_name)\n",
    "#     if os.path.isdir(folder_path):  # Check if it's a folder\n",
    "#         label = int(folder_name)  # Assume folder name is the label\n",
    "        \n",
    "#         # Load and preprocess each image in the folder\n",
    "#         for file_name in os.listdir(folder_path):\n",
    "#             file_path = os.path.join(folder_path, file_name)\n",
    "#             if file_name.endswith(\".png\"):  # Load only .png files\n",
    "#                 try:\n",
    "#                     img = Image.open(file_path).convert(\"RGB\")  # Convert to RGB\n",
    "#                     img = img.resize(IMG_SIZE)  # Resize the image\n",
    "#                     img_array = np.array(img, dtype=np.float32) / 255.0  # Normalize to [0, 1]\n",
    "                    \n",
    "#                     # Append preprocessed image and label to the data list\n",
    "#                     data.append((img_array, label))\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "# # Convert to NumPy arrays if needed for training\n",
    "# images, labels = zip(*data)  # Unzip into separate lists\n",
    "# images = np.array(images)  # Convert list of images to a NumPy array\n",
    "# labels = np.array(labels)  # Convert list of labels to a NumPy array\n",
    "\n",
    "# # Print summary\n",
    "# print(f\"Loaded {len(images)} images with shape {images[0].shape} and {len(labels)} labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8753ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.514519Z",
     "iopub.status.busy": "2024-12-04T07:40:21.514133Z",
     "iopub.status.idle": "2024-12-04T07:40:21.520275Z",
     "shell.execute_reply": "2024-12-04T07:40:21.519274Z"
    },
    "papermill": {
     "duration": 0.038901,
     "end_time": "2024-12-04T07:40:21.522440",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.483539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Convert labels to one-hot encoding (assuming labels are integers)\n",
    "# labels = to_categorical(labels)\n",
    "\n",
    "# # Split the data into training and validation sets (80% train, 20% validation)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Build the CNN model\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add convolutional layers\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# # Flatten the 3D output to 1D\n",
    "# model.add(Flatten())\n",
    "\n",
    "# # Add fully connected layers\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "\n",
    "# model.add(Dense(labels.shape[1], activation='softmax'))  # Output layer (softmax for multi-class classification)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Optionally, you can plot the training history (accuracy and loss)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Model Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('Model Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ffe136e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.727894Z",
     "iopub.status.busy": "2024-12-04T07:40:21.727488Z",
     "iopub.status.idle": "2024-12-04T07:40:21.734272Z",
     "shell.execute_reply": "2024-12-04T07:40:21.733173Z"
    },
    "papermill": {
     "duration": 0.041161,
     "end_time": "2024-12-04T07:40:21.736478",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.695317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications import VGG19\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Convert labels to one-hot encoding (assuming labels are integers)\n",
    "# labels = to_categorical(labels)\n",
    "\n",
    "# # Split the data into training and validation sets (80% train, 20% validation)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Load the pre-trained VGG19 model without the top (classifier) layers\n",
    "# base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# # Freeze the layers of VGG19 to prevent training on them (only fine-tune the new layers)\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Add custom top layers (fully connected layers)\n",
    "# model = Sequential()\n",
    "\n",
    "# # Add the VGG19 base model\n",
    "# model.add(base_model)\n",
    "\n",
    "# # Add a Global Average Pooling layer\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# # Add a fully connected (dense) layer with ReLU activation\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# # Add dropout to reduce overfitting\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "# # Output layer with softmax activation (for multi-class classification)\n",
    "# model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "\n",
    "# # Compile the model with Adam optimizer and categorical crossentropy loss function\n",
    "# model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Print model summary\n",
    "# model.summary()\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_val, y_val)\n",
    "# print(f\"Validation accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# # Optionally, you can plot the training history (accuracy and loss)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot training & validation accuracy values\n",
    "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('VGG19 Model Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.title('VGG19 Model Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce563689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.799584Z",
     "iopub.status.busy": "2024-12-04T07:40:21.799225Z",
     "iopub.status.idle": "2024-12-04T07:40:21.803864Z",
     "shell.execute_reply": "2024-12-04T07:40:21.802824Z"
    },
    "papermill": {
     "duration": 0.039642,
     "end_time": "2024-12-04T07:40:21.805841",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.766199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Path to the directory you want to delete (including all its contents)\n",
    "# dir_path = '/kaggle/working/'\n",
    "\n",
    "# # Check if the directory exists and remove it (along with all files and subdirectories)\n",
    "# if os.path.exists(dir_path) and os.path.isdir(dir_path):\n",
    "#     shutil.rmtree(dir_path)\n",
    "#     print(f\"{dir_path} and all its contents have been removed.\")\n",
    "# else:\n",
    "#     print(f\"The directory {dir_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc991d7",
   "metadata": {
    "papermill": {
     "duration": 0.029149,
     "end_time": "2024-12-04T07:40:21.864172",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.835023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Log_mel_spectogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "802c35ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.925509Z",
     "iopub.status.busy": "2024-12-04T07:40:21.924655Z",
     "iopub.status.idle": "2024-12-04T07:40:21.931860Z",
     "shell.execute_reply": "2024-12-04T07:40:21.930789Z"
    },
    "papermill": {
     "duration": 0.040462,
     "end_time": "2024-12-04T07:40:21.933793",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.893331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to apply Laplacian noise\n",
    "# def apply_laplacian_noise(y, epsilon):\n",
    "#     sensitivity = 1\n",
    "#     scale = sensitivity / epsilon\n",
    "#     laplacian_noise = np.random.laplace(0, scale, len(y))\n",
    "#     return y + laplacian_noise\n",
    "\n",
    "# # Function to plot and show Log Mel Spectrogram without axis or numbers\n",
    "# def show_log_mel_spectrogram(file_path, epsilon=1000, sr=None):\n",
    "#     y, sr = librosa.load(file_path, sr=sr)  # Load audio\n",
    "#     y_noisy = apply_laplacian_noise(y, epsilon)  # Apply Laplacian noise\n",
    "    \n",
    "#     mel_spec = librosa.feature.melspectrogram(y=y_noisy, sr=sr, n_mels=128, fmax=8000)  # Compute Mel spectrogram\n",
    "#     log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale (dB)\n",
    "\n",
    "#     # Plot the spectrogram without axes or numbers\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  # Show the spectrogram image without saving\n",
    "\n",
    "# # Function to plot and save Log Mel Spectrogram\n",
    "# def save_log_mel_spectrogram(file_path, save_dir, speaker_id, epsilon=1000):\n",
    "#     y, sr = librosa.load(file_path, sr=None)  # Load audio\n",
    "#     y_noisy = apply_laplacian_noise(y, epsilon)  # Apply Laplacian noise\n",
    "    \n",
    "#     mel_spec = librosa.feature.melspectrogram(y=y_noisy, sr=sr, n_mels=128, fmax=8000)  # Compute Mel spectrogram\n",
    "#     log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to log scale (dB)\n",
    "    \n",
    "#     # Create the speaker's directory if it doesn't exist\n",
    "#     speaker_dir = os.path.join(save_dir, speaker_id)\n",
    "#     os.makedirs(speaker_dir, exist_ok=True)\n",
    "    \n",
    "#     # Generate the filename for the spectrogram image\n",
    "#     spectrogram_filename = os.path.splitext(os.path.basename(file_path))[0] + \".png\"\n",
    "#     save_path = os.path.join(speaker_dir, spectrogram_filename)\n",
    "    \n",
    "#     # Plot and save the spectrogram without numbers or axis\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(log_mel_spec, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300)\n",
    "#     plt.close()  # Close the figure to free memory\n",
    "\n",
    "# # Load dataset and show/save spectrograms\n",
    "# def process_and_save_spectrograms(dataset_path, save_dir, epsilon=1000):\n",
    "#     image_count = 0  # Counter to show two images\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "                    \n",
    "#                     # Show two images before saving\n",
    "#                     if image_count < 2:\n",
    "#                         show_log_mel_spectrogram(file_path)  # Visualize the original\n",
    "#                         image_count += 1\n",
    "                    \n",
    "#                     # Save the spectrogram with noise applied\n",
    "#                     save_log_mel_spectrogram(file_path, save_dir, speaker_id, epsilon)\n",
    "\n",
    "# # Define paths\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# save_dir = \"/kaggle/working/spectrograms_e=1000\"  # Change this path if needed\n",
    "\n",
    "# # Process and save spectrograms\n",
    "# process_and_save_spectrograms(dataset_path, save_dir, epsilon=1000)\n",
    "\n",
    "# print(f\"Log Mel Spectrograms saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ba54cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:21.995509Z",
     "iopub.status.busy": "2024-12-04T07:40:21.994515Z",
     "iopub.status.idle": "2024-12-04T07:40:21.999466Z",
     "shell.execute_reply": "2024-12-04T07:40:21.998526Z"
    },
    "papermill": {
     "duration": 0.038143,
     "end_time": "2024-12-04T07:40:22.001547",
     "exception": false,
     "start_time": "2024-12-04T07:40:21.963404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Define the path to the folder and the output zip file\n",
    "# folder_path = '/kaggle/working/spectrograms_e=1000'\n",
    "# zip_file_path = '/kaggle/working/spectrograms_e=1000'\n",
    "\n",
    "# # Zip the folder\n",
    "# shutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n",
    "\n",
    "# print(f\"Folder '{folder_path}' has been zipped to '{zip_file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77334ca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:22.062898Z",
     "iopub.status.busy": "2024-12-04T07:40:22.062520Z",
     "iopub.status.idle": "2024-12-04T07:40:22.067626Z",
     "shell.execute_reply": "2024-12-04T07:40:22.066315Z"
    },
    "papermill": {
     "duration": 0.039258,
     "end_time": "2024-12-04T07:40:22.069999",
     "exception": false,
     "start_time": "2024-12-04T07:40:22.030741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from IPython.display import FileLink\n",
    "# FileLink(r'/kaggle/working/spectrograms.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe88f6",
   "metadata": {
    "papermill": {
     "duration": 0.030168,
     "end_time": "2024-12-04T07:40:22.130927",
     "exception": false,
     "start_time": "2024-12-04T07:40:22.100759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b2fdca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:22.192476Z",
     "iopub.status.busy": "2024-12-04T07:40:22.192083Z",
     "iopub.status.idle": "2024-12-04T07:40:22.198672Z",
     "shell.execute_reply": "2024-12-04T07:40:22.197551Z"
    },
    "papermill": {
     "duration": 0.039838,
     "end_time": "2024-12-04T07:40:22.200793",
     "exception": false,
     "start_time": "2024-12-04T07:40:22.160955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to plot and show MFCC Spectrogram without axis or numbers\n",
    "# def show_mfcc_spectrogram(file_path, sr=None):\n",
    "#     y, sr = librosa.load(file_path, sr=sr)  # Load audio\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute MFCC features (13 coefficients)\n",
    "    \n",
    "#     # Plot the MFCC spectrogram without axes or numbers\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(mfcc, sr=sr, x_axis='time', cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()  # Show the MFCC image without saving\n",
    "\n",
    "# # Function to plot and save MFCC Spectrogram\n",
    "# def save_mfcc_spectrogram(file_path, save_dir, speaker_id):\n",
    "#     y, sr = librosa.load(file_path, sr=None)  # Load audio\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # Compute MFCC features (13 coefficients)\n",
    "    \n",
    "#     # Create the speaker's directory if it doesn't exist\n",
    "#     speaker_dir = os.path.join(save_dir, speaker_id)\n",
    "#     os.makedirs(speaker_dir, exist_ok=True)\n",
    "    \n",
    "#     # Generate the filename for the MFCC image\n",
    "#     mfcc_filename = os.path.splitext(os.path.basename(file_path))[0] + \".png\"\n",
    "#     save_path = os.path.join(speaker_dir, mfcc_filename)\n",
    "    \n",
    "#     # Plot and save the MFCC spectrogram without numbers or axis\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     librosa.display.specshow(mfcc, sr=sr, x_axis='time', cmap='viridis')\n",
    "#     plt.axis('off')  # Hide the axis\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(save_path, dpi=300)\n",
    "#     plt.close()  # Close the figure to free memory\n",
    "\n",
    "# # Load dataset and show/save MFCC spectrograms\n",
    "# def process_and_save_mfcc_spectrograms(dataset_path, save_dir):\n",
    "#     image_count = 0  # Counter to show two images\n",
    "#     for phrase_type in [\"samePhrase\", \"differentPhrase\"]:\n",
    "#         phrase_path = os.path.join(dataset_path, phrase_type)\n",
    "#         if not os.path.exists(phrase_path):\n",
    "#             continue\n",
    "#         for speaker_id in os.listdir(phrase_path):\n",
    "#             speaker_path = os.path.join(phrase_path, speaker_id)\n",
    "#             if not os.path.isdir(speaker_path):\n",
    "#                 continue\n",
    "#             for audio_file in os.listdir(speaker_path):\n",
    "#                 if audio_file.endswith(\".flac\"):\n",
    "#                     file_path = os.path.join(speaker_path, audio_file)\n",
    "                    \n",
    "#                     # Show two images before saving\n",
    "#                     if image_count < 2:\n",
    "#                         show_mfcc_spectrogram(file_path)\n",
    "#                         image_count += 1\n",
    "                    \n",
    "#                     # Save the MFCC spectrogram\n",
    "#                     save_mfcc_spectrogram(file_path, save_dir, speaker_id)\n",
    "\n",
    "# # Define paths\n",
    "# dataset_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2\"\n",
    "# save_dir = \"/kaggle/working/mfcc_spectrograms\"  # Change this path if needed\n",
    "\n",
    "# # Process and save MFCC spectrograms\n",
    "# process_and_save_mfcc_spectrograms(dataset_path, save_dir)\n",
    "\n",
    "# print(f\"MFCC Spectrograms saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6aed4f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:22.262630Z",
     "iopub.status.busy": "2024-12-04T07:40:22.262251Z",
     "iopub.status.idle": "2024-12-04T07:40:22.267006Z",
     "shell.execute_reply": "2024-12-04T07:40:22.265930Z"
    },
    "papermill": {
     "duration": 0.03802,
     "end_time": "2024-12-04T07:40:22.269061",
     "exception": false,
     "start_time": "2024-12-04T07:40:22.231041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Encode labels\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_encoded = label_encoder.fit_transform(y)\n",
    "# y_encoded = tf.keras.utils.to_categorical(y_encoded)\n",
    "\n",
    "# # Save label encoder classes\n",
    "# np.save(\"label_classes.npy\", label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebd0800e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:22.330674Z",
     "iopub.status.busy": "2024-12-04T07:40:22.330271Z",
     "iopub.status.idle": "2024-12-04T07:40:22.336152Z",
     "shell.execute_reply": "2024-12-04T07:40:22.335168Z"
    },
    "papermill": {
     "duration": 0.039337,
     "end_time": "2024-12-04T07:40:22.338126",
     "exception": false,
     "start_time": "2024-12-04T07:40:22.298789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import librosa\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Parameters\n",
    "# SR = 16000  # Sampling rate\n",
    "# MFCC_FEATURES = 40  # Number of MFCC features\n",
    "# MAX_LEN = 100  # Sequence length for padding/truncation\n",
    "\n",
    "# # Load the trained model\n",
    "# model = load_model(\"/kaggle/working/speaker_identity_recognition_model.h5\")\n",
    "\n",
    "# # Load the label encoder classes\n",
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.classes_ = np.load(\"/kaggle/working/label_classes.npy\", allow_pickle=True)  # Load from saved file\n",
    "\n",
    "# # Feature extraction function\n",
    "# def extract_features(file_path):\n",
    "#     try:\n",
    "#         audio, sr = librosa.load(file_path, sr=SR)\n",
    "#         mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=MFCC_FEATURES)\n",
    "#         # Pad or truncate\n",
    "#         if mfcc.shape[1] > MAX_LEN:\n",
    "#             mfcc = mfcc[:, :MAX_LEN]\n",
    "#         else:\n",
    "#             mfcc = np.pad(mfcc, ((0, 0), (0, MAX_LEN - mfcc.shape[1])), mode=\"constant\")\n",
    "#         return mfcc.T\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def predict_speaker(audio_path):\n",
    "#     features = extract_features(audio_path)\n",
    "#     if features is None:\n",
    "#         raise ValueError(\"Failed to extract features from the audio file.\")\n",
    "    \n",
    "#     features = np.expand_dims(features, axis=0)  # Add batch dimension\n",
    "#     prediction = model.predict(features)  # Predict using the trained model\n",
    "    \n",
    "#     # Get the class with the highest probability (argmax) and inverse transform it\n",
    "#     speaker_id = label_encoder.inverse_transform([np.argmax(prediction)])  \n",
    "#     return speaker_id[0]\n",
    "\n",
    "# # Test the prediction function\n",
    "# audio_path = \"/kaggle/input/d/mdalhadikhan/a-dataset-for-voice-based-human-identity/zw4p4p7sdh-2/samePhrase/11/11-1.flac\"  # Replace with the path to your audio file\n",
    "\n",
    "# try:\n",
    "#     predicted_speaker = predict_speaker(audio_path)\n",
    "#     print(f\"Predicted Speaker ID: {predicted_speaker}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during prediction: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da951fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:22.399412Z",
     "iopub.status.busy": "2024-12-04T07:40:22.399047Z",
     "iopub.status.idle": "2024-12-04T07:40:22.403782Z",
     "shell.execute_reply": "2024-12-04T07:40:22.402818Z"
    },
    "papermill": {
     "duration": 0.038013,
     "end_time": "2024-12-04T07:40:22.405780",
     "exception": false,
     "start_time": "2024-12-04T07:40:22.367767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f2c5ce3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T07:40:22.466974Z",
     "iopub.status.busy": "2024-12-04T07:40:22.466550Z",
     "iopub.status.idle": "2024-12-04T07:40:22.475034Z",
     "shell.execute_reply": "2024-12-04T07:40:22.473917Z"
    },
    "papermill": {
     "duration": 0.041834,
     "end_time": "2024-12-04T07:40:22.477115",
     "exception": false,
     "start_time": "2024-12-04T07:40:22.435281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import librosa\n",
    "# import numpy as np\n",
    "# import soundfile as sf\n",
    "# import parselmouth\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract audio features from the audio file\n",
    "# def extract_audio_features(audio_path, sr=16000, duration=10, max_length=300):\n",
    "#     # Load audio file\n",
    "#     y, sr = librosa.load(audio_path, sr=sr, duration=duration)\n",
    "    \n",
    "#     # 1. MFCCs and Deltas\n",
    "#     mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "#     delta_mfccs = librosa.feature.delta(mfccs)\n",
    "#     delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "    \n",
    "#     # Combine MFCC features\n",
    "#     mfcc_features = np.vstack([mfccs, delta_mfccs, delta2_mfccs])\n",
    "    \n",
    "#     # 2. Pitch (Fundamental Frequency)\n",
    "#     pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "#     pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0  # Mean pitch\n",
    "#     pitch_features = np.full((1, mfcc_features.shape[1]), pitch_mean)  # Match time axis\n",
    "    \n",
    "#     # 3. Energy (RMS)\n",
    "#     rms = librosa.feature.rms(y=y).flatten()\n",
    "#     rms_features = np.expand_dims(rms, axis=0)  # Add channel dimension\n",
    "    \n",
    "#     # 4. Log-Mel Spectrogram\n",
    "#     mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "#     log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "#     log_mel_features = log_mel_spec[:max_length, :]  # Truncate or pad for uniform size\n",
    "    \n",
    "#     # 5. Formants (F1, F2, F3)\n",
    "#     sound = parselmouth.Sound(audio_path)\n",
    "#     formant = sound.to_formant_burg()\n",
    "#     formants = []\n",
    "#     for t in np.linspace(0, sound.duration, num=max_length):\n",
    "#         try:\n",
    "#             f1 = formant.get_value_at_time(1, t)\n",
    "#             f2 = formant.get_value_at_time(2, t)\n",
    "#             f3 = formant.get_value_at_time(3, t)\n",
    "#             formants.append([f1, f2, f3])\n",
    "#         except Exception:\n",
    "#             formants.append([0, 0, 0])  # Use 0 for missing values\n",
    "    \n",
    "#     formants = np.array(formants).T  # Shape (3, max_length)\n",
    "    \n",
    "#     # Padding or truncating all features to the same length (max_length)\n",
    "#     mfcc_features = np.pad(mfcc_features, ((0, 0), (0, max_length - mfcc_features.shape[1])), mode='constant')\n",
    "#     pitch_features = np.pad(pitch_features, ((0, 0), (0, max_length - pitch_features.shape[1])), mode='constant')\n",
    "#     rms_features = np.pad(rms_features, ((0, 0), (0, max_length - rms_features.shape[1])), mode='constant')\n",
    "#     log_mel_features = np.pad(log_mel_features, ((0, 0), (0, max_length - log_mel_features.shape[1])), mode='constant')\n",
    "#     formants = np.pad(formants, ((0, 0), (0, max_length - formants.shape[1])), mode='constant')\n",
    "    \n",
    "#     # Combine all features along the frequency axis\n",
    "#     combined_features = np.vstack([mfcc_features, pitch_features, rms_features, log_mel_features, formants])\n",
    "    \n",
    "#     return combined_features\n",
    "\n",
    "# # Function to plot features for two speaker IDs\n",
    "# def plot_features_for_two_ids(audio_path1, audio_path2, sr=16000, duration=10, max_length=300):\n",
    "#     # Extract features for both audio paths\n",
    "#     features1 = extract_audio_features(audio_path1, sr=sr, duration=duration, max_length=max_length)\n",
    "#     features2 = extract_audio_features(audio_path2, sr=sr, duration=duration, max_length=max_length)\n",
    "\n",
    "#     # Feature categories\n",
    "#     feature_types = [\n",
    "#         \"MFCCs and Deltas\",\n",
    "#         \"Pitch (Mean)\",\n",
    "#         \"Energy (RMS)\",\n",
    "#         \"Log-Mel Spectrogram\",\n",
    "#         \"Formants (F1, F2, F3)\",\n",
    "#     ]\n",
    "\n",
    "#     # Define feature slices\n",
    "#     mfcc_dim = 39  # 13 MFCC + 13 Deltas + 13 Delta-Deltas\n",
    "#     pitch_dim = 1\n",
    "#     rms_dim = 1\n",
    "#     mel_dim = 128\n",
    "#     formant_dim = 3\n",
    "\n",
    "#     # Slice indices\n",
    "#     slices = [\n",
    "#         (0, mfcc_dim),  # MFCCs\n",
    "#         (mfcc_dim, mfcc_dim + pitch_dim),  # Pitch\n",
    "#         (mfcc_dim + pitch_dim, mfcc_dim + pitch_dim + rms_dim),  # RMS\n",
    "#         (mfcc_dim + pitch_dim + rms_dim, mfcc_dim + pitch_dim + rms_dim + mel_dim),  # Log-Mel\n",
    "#         (mfcc_dim + pitch_dim + rms_dim + mel_dim, None),  # Formants\n",
    "#     ]\n",
    "\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(len(feature_types), 1, figsize=(12, 20))\n",
    "#     fig.tight_layout(pad=5.0)\n",
    "\n",
    "#     for i, (feature_type, (start, end)) in enumerate(zip(feature_types, slices)):\n",
    "#         feat1 = features1[start:end, :]\n",
    "#         feat2 = features2[start:end, :]\n",
    "\n",
    "#         # Plot features\n",
    "#         if feature_type == \"Log-Mel Spectrogram\":\n",
    "#             axs[i].imshow(feat1, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\", alpha=0.7)\n",
    "#             axs[i].imshow(feat2, aspect=\"auto\", origin=\"lower\", cmap=\"coolwarm\", alpha=0.7)\n",
    "#             axs[i].set_title(f\"{feature_type} (Overlap Visualization)\")\n",
    "#         elif feature_type == \"Formants (F1, F2, F3)\":\n",
    "#             for j, formant in enumerate([\"F1\", \"F2\", \"F3\"]):\n",
    "#                 axs[i].plot(feat1[j, :], label=f\"Speaker 1 - {formant}\")\n",
    "#                 axs[i].plot(feat2[j, :], label=f\"Speaker 2 - {formant}\", linestyle=\"dashed\")\n",
    "#             axs[i].legend()\n",
    "#         else:\n",
    "#             axs[i].plot(feat1.flatten(), label=\"Speaker 1\")\n",
    "#             axs[i].plot(feat2.flatten(), label=\"Speaker 2\", linestyle=\"dashed\")\n",
    "#             axs[i].legend()\n",
    "\n",
    "#         axs[i].set_title(feature_type)\n",
    "#         axs[i].set_xlabel(\"Time Frames\")\n",
    "#         axs[i].set_ylabel(\"Feature Values\")\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Example usage\n",
    "# audio_path1 = all_audio_paths[0]  # Replace with an actual path from a specific speaker ID\n",
    "# audio_path2 = all_audio_paths[1]  # Replace with another speaker ID\n",
    "# plot_features_for_two_ids(audio_path1, audio_path2)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 287521,
     "sourceId": 591084,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1063054,
     "sourceId": 1788543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6009691,
     "sourceId": 9804778,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6178775,
     "sourceId": 10032036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6178957,
     "sourceId": 10032278,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186604,
     "sourceId": 10042684,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186791,
     "sourceId": 10042937,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186849,
     "sourceId": 10043006,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186907,
     "sourceId": 10043086,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6186947,
     "sourceId": 10043145,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6187051,
     "sourceId": 10043291,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6187347,
     "sourceId": 10043690,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7585.058442,
   "end_time": "2024-12-04T07:40:23.834330",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-04T05:33:58.775888",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
